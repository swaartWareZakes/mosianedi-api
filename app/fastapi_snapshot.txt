--- ./routers/provincial_stats.py ---
from fastapi import APIRouter, HTTPException, Depends, UploadFile, File
from pydantic import BaseModel
from typing import List, Optional
from uuid import UUID
import pandas as pd
import io
from app.routers.projects import get_db_connection, get_current_user_id

router = APIRouter()

# --- SCHEMAS --------------------------------------------------------
class ProvincialStatUpdate(BaseModel):
    province_name: str
    km_arid: Optional[float] = 0
    km_semi_arid: Optional[float] = 0
    km_dry_sub_humid: Optional[float] = 0
    km_moist_sub_humid: Optional[float] = 0
    km_humid: Optional[float] = 0
    avg_vci: Optional[float] = 0
    vehicle_km: Optional[float] = 0
    fuel_sales: Optional[float] = 0

class ProvincialStatResponse(ProvincialStatUpdate):
    id: UUID
    project_id: UUID

# --- ENDPOINTS ------------------------------------------------------

@router.get("/{project_id}", response_model=List[ProvincialStatResponse])
def get_stats(project_id: UUID, user_id: str = Depends(get_current_user_id)):
    """
    Returns the 9 rows. They are guaranteed to exist because of the SQL trigger.
    """
    sql = """
        SELECT * FROM public.provincial_stats
        WHERE project_id = %s
        ORDER BY province_name ASC;
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id),))
            rows = cur.fetchall()
            cols = [desc[0] for desc in cur.description]
            return [dict(zip(cols, row)) for row in rows]

@router.post("/{project_id}", status_code=200)
def save_manual_input(
    project_id: UUID, 
    stats: List[ProvincialStatUpdate], 
    user_id: str = Depends(get_current_user_id)
):
    """
    Saves user edits from the grid.
    Uses 'UPDATE' because the rows already exist.
    """
    sql = """
        UPDATE public.provincial_stats
        SET 
            km_arid = %s, km_semi_arid = %s, km_dry_sub_humid = %s, 
            km_moist_sub_humid = %s, km_humid = %s,
            avg_vci = %s, vehicle_km = %s, fuel_sales = %s,
            updated_at = NOW()
        WHERE project_id = %s AND province_name = %s;
    """
    
    values = []
    for s in stats:
        values.append((
            s.km_arid, s.km_semi_arid, s.km_dry_sub_humid, 
            s.km_moist_sub_humid, s.km_humid,
            s.avg_vci, s.vehicle_km, s.fuel_sales,
            str(project_id), s.province_name
        ))

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            # Execute batch update
            cur.executemany(sql, values)
            conn.commit()
            
    return {"message": "Saved successfully"}

@router.post("/{project_id}/upload")
def upload_stats_csv(
    project_id: UUID, 
    file: UploadFile = File(...), 
    user_id: str = Depends(get_current_user_id)
):
    """
    Parses 'Book1.csv' and updates the existing 9 rows.
    """
    try:
        contents = file.file.read()
        if file.filename.endswith('.csv'):
            df = pd.read_csv(io.BytesIO(contents), header=None)
        else:
            df = pd.read_excel(io.BytesIO(contents), header=None)

        # Basic Cleanup tailored to your CSV structure
        df = df.iloc[3:] # Skip headers
        df = df[df[0].notna()] # Remove empty lines
        
        updates = []
        for _, row in df.iterrows():
            prov_name = str(row[0]).strip()
            # Skip totals/junk
            if "Total" in prov_name or "Province" in prov_name:
                continue

            def parse(val):
                if isinstance(val, (int, float)): return val
                if isinstance(val, str):
                    clean = val.replace('R','').replace(',','').replace(' ','').replace('%','')
                    try: return float(clean)
                    except: return 0
                return 0

            # Map CSV Columns to DB Columns
            updates.append((
                parse(row[1]), parse(row[2]), parse(row[3]), parse(row[4]), parse(row[5]), # Climate Kms
                parse(row[7]), parse(row[8]), parse(row[10]), # VCI, VehKm, Fuel
                str(project_id), prov_name # WHERE clause
            ))

        sql = """
            UPDATE public.provincial_stats
            SET 
                km_arid=%s, km_semi_arid=%s, km_dry_sub_humid=%s, km_moist_sub_humid=%s, km_humid=%s,
                avg_vci=%s, vehicle_km=%s, fuel_sales=%s, updated_at=NOW()
            WHERE project_id=%s AND province_name=%s;
        """

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.executemany(sql, updates)
                conn.commit()

        return {"message": "Spreadsheet data imported"}

    except Exception as e:
        raise HTTPException(400, f"Upload Error: {str(e)}")--- ./routers/__init__.py ---
--- ./routers/master_data.py ---
"""
Master Data Upload, Validation, History & Preview Endpoints
===========================================================

Clean, modular, production-ready version.
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from typing import Optional, Dict, Any, List
from uuid import UUID
from io import BytesIO

import pandas as pd
import psycopg2
from psycopg2.extras import Json

from pydantic import BaseModel

# Shared helpers
from app.routers.projects import get_db_connection, get_current_user_id
from app.db.schemas import MasterDataUploadStatus


# ============================================================
# PREVIEW SCHEMAS
# ============================================================

class DataPreviewResponse(BaseModel):
    preview_data: List[Dict[str, Any]]
    total_rows: int
    columns: List[str]


# ============================================================
# REQUIRED COLUMNS FOR MASTER DATA
# ============================================================

REQUIRED_COLUMNS = {
    "segment_id",
    "road_id",
    "road_class",
    "length_km",
    "surface_type",
}

router = APIRouter()


# ============================================================
# HELPERS
# ============================================================

def _parse_master_data_file(file_bytes: bytes, filename: str) -> pd.DataFrame:
    """
    Convert uploaded Excel/CSV file â†’ pandas DataFrame.
    """
    buffer = BytesIO(file_bytes)
    name = filename.lower()

    try:
        if name.endswith((".xlsx", ".xls")):
            return pd.read_excel(buffer)
        elif name.endswith(".csv"):
            return pd.read_csv(buffer)
        else:
            raise HTTPException(
                status_code=400,
                detail="Invalid file type. Use .xlsx, .xls or .csv."
            )
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail=f"Could not parse file: {str(e)}"
        )


# ============================================================
# UPLOAD ENDPOINT
# ============================================================

@router.post("/{project_id}/master-data/upload", response_model=MasterDataUploadStatus)
async def upload_master_data(
    project_id: UUID,
    file: UploadFile = File(...),
    user_id: str = Depends(get_current_user_id),
):
    """
    Upload + validate a master data file.
    Saves the file blob inline in PostgreSQL.
    """
    file_bytes = await file.read()
    if not file_bytes:
        raise HTTPException(status_code=400, detail="Uploaded file is empty.")

    mime_type = file.content_type or "application/octet-stream"
    file_size = len(file_bytes)

    validation_errors: Dict[str, Any] = {}
    status = "validated"
    row_count: Optional[int] = None

    # ---- VALIDATION ----
    try:
        df = _parse_master_data_file(file_bytes, file.filename)
        row_count = len(df.index)

        df_columns_lower = {c.lower(): c for c in df.columns}
        missing = [col for col in REQUIRED_COLUMNS if col not in df_columns_lower]

        if missing:
            status = "failed"
            validation_errors["missing_columns"] = missing

    except HTTPException:
        status = "failed"
        raise
    except Exception as e:
        status = "failed"
        validation_errors["validation_error"] = f"Unexpected: {str(e)}"

    # ---- INSERT INTO DB ----
    sql = """
        INSERT INTO public.master_data_uploads (
            project_id, user_id,
            original_filename, mime_type, file_size,
            storage_strategy, file_blob,
            status, row_count, validation_errors
        )
        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
        RETURNING
            id, project_id, user_id, original_filename, mime_type,
            file_size, status, row_count, validation_errors, created_at;
    """

    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (
                    str(project_id),
                    user_id,
                    file.filename,
                    mime_type,
                    file_size,
                    "inline",
                    psycopg2.Binary(file_bytes),
                    status,
                    row_count,
                    Json(validation_errors) if validation_errors else None
                ))

                record = dict(zip([d[0] for d in cur.description], cur.fetchone()))
                conn.commit()
                return record

    except Exception as e:
        print("[MASTER_DATA] Upload DB error:", e)
        raise HTTPException(500, "Internal server error during upload.")


# ============================================================
# LAST UPLOAD ENDPOINT
# ============================================================

@router.get("/{project_id}/master-data/last-upload", response_model=MasterDataUploadStatus)
async def get_last_master_data_upload(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id)
):
    """
    Return metadata for the most recent upload.
    """
    sql = """
        SELECT
            id, project_id, user_id,
            original_filename, mime_type, file_size,
            status, row_count, validation_errors, created_at
        FROM public.master_data_uploads
        WHERE project_id = %s AND user_id = %s
        ORDER BY created_at DESC
        LIMIT 1;
    """

    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (str(project_id), user_id))
                row = cur.fetchone()

                if not row:
                    raise HTTPException(404, "No master data uploads found.")

                record = dict(zip([d[0] for d in cur.description], row))
                return record

    except Exception as e:
        print("[MASTER_DATA] Last upload fetch error:", e)
        raise HTTPException(500, "Internal server error fetching last upload.")


# ============================================================
# PREVIEW ENDPOINT (LATEST FILE)
# ============================================================

@router.get("/{project_id}/master-data/preview", response_model=DataPreviewResponse)
async def get_master_data_preview(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id)
):
    """
    Reads the file_blobs of the most recent upload and returns 
    up to 50 preview rows.
    """
    sql = """
        SELECT file_blob, original_filename
        FROM public.master_data_uploads
        WHERE project_id = %s AND user_id = %s
        ORDER BY created_at DESC
        LIMIT 1;
    """

    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (str(project_id), user_id))
                row = cur.fetchone()

                if not row:
                    raise HTTPException(404, "No file found for preview.")

                file_blob_raw, filename = row

                if isinstance(file_blob_raw, memoryview):
                    file_bytes = file_blob_raw.tobytes()
                else:
                    file_bytes = file_blob_raw

    except Exception as e:
        print("[PREVIEW] DB error:", e)
        raise HTTPException(500, "Database error fetching file for preview.")

    # ---- PARSE ----
    try:
        df = _parse_master_data_file(file_bytes, filename)
        total_rows = len(df.index)

        preview_df = df.head(50).fillna("")
        preview_data = preview_df.to_dict("records")

        return DataPreviewResponse(
            preview_data=preview_data,
            total_rows=total_rows,
            columns=list(df.columns)
        )
    except Exception as e:
        print("[PREVIEW] Pandas error:", e)
        raise HTTPException(500, f"Error processing preview: {str(e)}")--- ./routers/projects.py ---
from fastapi import APIRouter, HTTPException, Depends, Header
from decimal import Decimal
import os
import psycopg2
from contextlib import contextmanager
from jose import jwt, JWTError
from uuid import UUID  # <--- THIS WAS MISSING

from app.db.schemas import ProjectMetadata, ProjectDB 

router = APIRouter()

JWT_SECRET = os.getenv("SUPABASE_JWT_SECRET")
ALGORITHM = "HS256"

@contextmanager
def get_db_connection():
    DB_URL = os.getenv("DATABASE_URL")
    if not DB_URL:
        raise ValueError("DATABASE_URL missing")

    conn = None
    try:
        conn = psycopg2.connect(DB_URL)
        yield conn
    finally:
        if conn:
            conn.close()

def get_current_user_id(authorization: str = Header(None)) -> str:
    if not authorization:
        raise HTTPException(401, "Authorization header missing")
    try:
        scheme, token = authorization.split()
        payload = jwt.decode(token, JWT_SECRET, algorithms=[ALGORITHM], options={"verify_aud": False})
        return payload.get("sub")
    except:
        raise HTTPException(401, "Invalid token")

@router.post("/", status_code=201)
async def create_project(metadata: ProjectMetadata, user_id: str = Depends(get_current_user_id)):
    sql = """
        INSERT INTO public.projects 
        (user_id, project_name, description, start_year, forecast_duration, discount_rate)
        VALUES (%s, %s, %s, %s, %s, %s)
        RETURNING id, created_at;
    """

    data = (
        user_id,
        metadata.project_name,
        metadata.description,
        metadata.start_year,
        metadata.forecast_duration,
        Decimal(str(metadata.discount_rate)),
    )

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, data)
            project_id, created_at = cur.fetchone()
            conn.commit()

    return {
        "project_id": str(project_id),
        "message": "Project created",
        "created_at": created_at.isoformat(),
    }
    
@router.get(
    "/{project_id}",
    response_model=ProjectDB,
    summary="Get details of a specific project",
)
def get_project(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    sql = """
        SELECT * FROM public.projects
        WHERE id = %s AND user_id = %s;
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            row = cur.fetchone()
            
            if not row:
                raise HTTPException(status_code=404, detail="Project not found")
                
            # Convert row to dict
            cols = [desc[0] for desc in cur.description]
            return dict(zip(cols, row))
        
        

@router.get("/", response_model=list[ProjectDB])
async def list_projects(user_id: str = Depends(get_current_user_id)):
    sql = """
        SELECT id, user_id, project_name, description, start_year, forecast_duration, 
               discount_rate, created_at, updated_at
        FROM public.projects
        WHERE user_id = %s
        ORDER BY created_at DESC;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (user_id,))
            columns = [c[0] for c in cur.description]
            rows = cur.fetchall()

    projects = []
    for row in rows:
        d = dict(zip(columns, row))
        if isinstance(d["discount_rate"], Decimal):
            d["discount_rate"] = float(d["discount_rate"])
        projects.append(d)

    return projects--- ./__init__.py ---
--- ./network_snapshot/service.py ---
# app/network_snapshot/service.py

from uuid import UUID
from typing import Optional, Tuple, Any, List, Dict

import json
import pandas as pd
from fastapi import HTTPException

from app.routers.projects import get_db_connection
from app.master_data.validation import parse_master_data_file  # fallback for legacy uploads
from .schemas import (
    NetworkSnapshot,
    LengthByCategory,
    AssetValueByCategory,
    UnitCostByCategory,
)


# -------------------------------------------------
# DB helper: latest upload record + workbook blob
# -------------------------------------------------


def _fetch_latest_master_data_record(
    project_id: UUID,
    user_id: str,
) -> Tuple[UUID, Optional[Dict[str, Any]], Optional[bytes], Optional[str]]:
    """
    Fetch the latest master_data_uploads row for this project+user.

    Returns:
        upload_id,
        workbook_payload (dict or None),
        file_bytes (from file_blob, or None),
        original_filename (or None)
    """
    sql = """
        SELECT
            id,
            workbook_payload,
            file_blob,
            original_filename
        FROM public.master_data_uploads
        WHERE project_id = %s AND user_id = %s
        ORDER BY created_at DESC
        LIMIT 1;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            row = cur.fetchone()

            if not row:
                raise HTTPException(
                    status_code=404,
                    detail="No master data uploads found for this project.",
                )

            upload_id, workbook_payload_raw, file_blob_raw, original_filename = row

            # workbook_payload is usually a dict (psycopg2 Json), but be defensive
            workbook_payload: Optional[Dict[str, Any]] = None
            if workbook_payload_raw is not None:
                if isinstance(workbook_payload_raw, dict):
                    workbook_payload = workbook_payload_raw
                else:
                    try:
                        workbook_payload = json.loads(workbook_payload_raw)
                    except Exception:
                        workbook_payload = None

            # Convert file_blob to bytes if present
            file_bytes: Optional[bytes] = None
            if file_blob_raw is not None:
                if isinstance(file_blob_raw, memoryview):
                    file_bytes = file_blob_raw.tobytes()
                else:
                    file_bytes = file_blob_raw

            return upload_id, workbook_payload, file_bytes, original_filename


# -------------------------------------------------
# Generic helpers
# -------------------------------------------------


def _compute_length_breakdown(
    df: pd.DataFrame,
    column: str,
) -> List[LengthByCategory]:
    """
    Group by a categorical column (e.g. road_class, surface_type)
    and sum length_km.
    """
    if column not in df.columns or "length_km" not in df.columns:
        return []

    grouped = (
        df.groupby(column)["length_km"]
        .sum()
        .reset_index()
        .sort_values("length_km", ascending=False)
    )

    results: List[LengthByCategory] = []
    for _, row in grouped.iterrows():
        label = str(row[column])
        length = float(row["length_km"] or 0)
        results.append(LengthByCategory(label=label, length_km=length))

    return results


def _get_segments_df_from_workbook(
    workbook_payload: Dict[str, Any]
) -> Optional[pd.DataFrame]:
    """
    Build a DataFrame from workbook_payload['segments'] if present.
    """
    segments = workbook_payload.get("segments")
    if not segments:
        return None

    df = pd.DataFrame(segments)
    df.columns = [str(c).strip().lower() for c in df.columns]

    if "length_km" in df.columns:
        df["length_km"] = pd.to_numeric(df["length_km"], errors="coerce").fillna(0)

    return df


def _get_sheet_df(
    workbook_payload: Dict[str, Any],
    sheet_name: str,
) -> Optional[pd.DataFrame]:
    """
    Small helper: safely convert a workbook sheet to DataFrame.
    """
    if not workbook_payload or sheet_name not in workbook_payload:
        return None

    data = workbook_payload.get(sheet_name)
    if not data:
        return None

    df = pd.DataFrame(data)
    df.columns = [str(c).strip().lower() for c in df.columns]
    return df


def _find_first_numeric_column(
    df: pd.DataFrame,
    preferred_names: List[str],
) -> Optional[str]:
    """
    Tries a list of preferred column names; if none exist, picks
    the first numeric column, else None.
    """
    cols = set(df.columns)

    for name in preferred_names:
        if name in cols:
            return name

    # Fallback: any numeric-looking column
    numeric_cols = df.select_dtypes(include=["number"]).columns
    if len(numeric_cols) > 0:
        return numeric_cols[0]

    # Try to coerce object columns
    for col in df.columns:
        try:
            pd.to_numeric(df[col], errors="raise")
            return col
        except Exception:
            continue

    return None


# -------------------------------------------------
# Extra metrics by sheet
# -------------------------------------------------


def _compute_network_length_metrics(
    workbook_payload: Optional[Dict[str, Any]],
) -> tuple[Optional[float], List[LengthByCategory]]:
    """
    Use 'network_length' sheet if present.
    """
    if not workbook_payload:
        return None, []

    df = _get_sheet_df(workbook_payload, "network_length")
    if df is None or df.empty:
        return None, []

    # Ensure numeric length column
    length_col = _find_first_numeric_column(
        df,
        preferred_names=["length_km", "network_length_km", "length"],
    )
    if not length_col:
        return None, []

    df[length_col] = pd.to_numeric(df[length_col], errors="coerce").fillna(0)

    total_network_length_km = float(df[length_col].sum())

    # Category for breakdown â€“ try a few expected options
    category_col = None
    for candidate in ["network_type", "road_class", "surface_type"]:
        if candidate in df.columns:
            category_col = candidate
            break

    length_by_network_type: List[LengthByCategory] = []
    if category_col:
        grouped = (
            df.groupby(category_col)[length_col]
            .sum()
            .reset_index()
            .sort_values(length_col, ascending=False)
        )
        for _, row in grouped.iterrows():
            label = str(row[category_col])
            length = float(row[length_col] or 0)
            length_by_network_type.append(
                LengthByCategory(label=label, length_km=length)
            )

    return total_network_length_km, length_by_network_type


def _compute_asset_value_metrics(
    workbook_payload: Optional[Dict[str, Any]],
) -> tuple[Optional[float], List[AssetValueByCategory]]:
    """
    Use 'asset_value' sheet if present.
    """
    if not workbook_payload:
        return None, []

    df = _get_sheet_df(workbook_payload, "asset_value")
    if df is None or df.empty:
        return None, []

    value_col = _find_first_numeric_column(
        df,
        preferred_names=["asset_value", "value", "total_value"],
    )
    if not value_col:
        return None, []

    df[value_col] = pd.to_numeric(df[value_col], errors="coerce").fillna(0)

    total_asset_value = float(df[value_col].sum())

    # Category â€“ pick something readable if present
    category_col = None
    for candidate in ["asset_type", "network_type", "road_class", "surface_type"]:
        if candidate in df.columns:
            category_col = candidate
            break

    asset_value_by_category: List[AssetValueByCategory] = []
    if category_col:
        grouped = (
            df.groupby(category_col)[value_col]
            .sum()
            .reset_index()
            .sort_values(value_col, ascending=False)
        )
        for _, row in grouped.iterrows():
            label = str(row[category_col])
            value = float(row[value_col] or 0)
            asset_value_by_category.append(
                AssetValueByCategory(label=label, value=value)
            )

    return total_asset_value, asset_value_by_category


def _compute_unit_cost_metrics(
    workbook_payload: Optional[Dict[str, Any]],
) -> List[UnitCostByCategory]:
    """
    Use 'road_costs' sheet if present.

    We look for a cost-per-km style column and group by surface_type or
    maintenance_type, depending on what exists.
    """
    if not workbook_payload:
        return []

    df = _get_sheet_df(workbook_payload, "road_costs")
    if df is None or df.empty:
        return []

    cost_col = _find_first_numeric_column(
        df,
        preferred_names=[
            "cost_per_km",
            "unit_cost_per_km",
            "unit_cost",
            "cost",
        ],
    )
    if not cost_col:
        return []

    df[cost_col] = pd.to_numeric(df[cost_col], errors="coerce").fillna(0)

    # Choose a category column
    category_col = None
    for candidate in ["surface_type", "treatment_type", "maintenance_type", "road_class"]:
        if candidate in df.columns:
            category_col = candidate
            break

    if not category_col:
        # No category â†’ summarise a single â€œallâ€ bucket
        avg_cost = float(df[cost_col].mean())
        return [
            UnitCostByCategory(label="All", cost_per_km=avg_cost),
        ]

    grouped = (
        df.groupby(category_col)[cost_col]
        .mean()
        .reset_index()
        .sort_values(cost_col, ascending=False)
    )

    results: List[UnitCostByCategory] = []
    for _, row in grouped.iterrows():
        label = str(row[category_col])
        cost = float(row[cost_col] or 0)
        results.append(UnitCostByCategory(label=label, cost_per_km=cost))

    return results


# -------------------------------------------------
# Public service
# -------------------------------------------------


def get_network_snapshot(
    project_id: UUID,
    user_id: str,
) -> NetworkSnapshot:
    """
    Compute a lightweight 'network snapshot' for the latest
    master data upload.

    Sources:
    - segments sheet      â†’ core totals + breakdowns
    - network_length      â†’ total_network_length_km, length_by_network_type
    - asset_value         â†’ total_asset_value, asset_value_by_category
    - road_costs          â†’ unit_costs_by_surface
    """
    # 1) Get latest upload record
    upload_id, workbook_payload, file_bytes, filename = _fetch_latest_master_data_record(
        project_id=project_id,
        user_id=user_id,
    )

    # 2) Get segments DataFrame
    df_segments: Optional[pd.DataFrame] = None
    if workbook_payload:
        df_segments = _get_segments_df_from_workbook(workbook_payload)

    # Fallback to blob (legacy uploads)
    if df_segments is None:
        if not file_bytes or not filename:
            raise HTTPException(
                status_code=500,
                detail="No usable workbook payload or file blob found for snapshot.",
            )
        df_segments = parse_master_data_file(file_bytes, filename)
        df_segments.columns = [str(c).strip().lower() for c in df_segments.columns]
        if "length_km" in df_segments.columns:
            df_segments["length_km"] = pd.to_numeric(
                df_segments["length_km"], errors="coerce"
            ).fillna(0)

    if "length_km" not in df_segments.columns:
        raise HTTPException(
            status_code=400,
            detail="The master data (segments) does not contain a 'length_km' column.",
        )

    # 3) Core metrics from segments
    total_segments = int(len(df_segments.index))
    total_length_km = float(df_segments["length_km"].sum())

    total_roads: Optional[int] = None
    if "road_id" in df_segments.columns:
        total_roads = int(df_segments["road_id"].nunique())

    length_by_road_class = (
        _compute_length_breakdown(df_segments, "road_class")
        if "road_class" in df_segments.columns
        else []
    )
    length_by_surface_type = (
        _compute_length_breakdown(df_segments, "surface_type")
        if "surface_type" in df_segments.columns
        else []
    )

    # 4) Extra metrics from other sheets
    total_network_length_km, length_by_network_type = _compute_network_length_metrics(
        workbook_payload
    )
    total_asset_value, asset_value_by_category = _compute_asset_value_metrics(
        workbook_payload
    )
    unit_costs_by_surface = _compute_unit_cost_metrics(workbook_payload)

    # 5) Build response
    return NetworkSnapshot(
        project_id=project_id,
        upload_id=upload_id,
        total_length_km=total_length_km,
        total_segments=total_segments,
        total_roads=total_roads,
        length_by_road_class=length_by_road_class,
        length_by_surface_type=length_by_surface_type,
        total_network_length_km=total_network_length_km,
        length_by_network_type=length_by_network_type,
        total_asset_value=total_asset_value,
        asset_value_by_category=asset_value_by_category,
        unit_costs_by_surface=unit_costs_by_surface,
    )--- ./network_snapshot/__init__.py ---
# app/network_snapshot/__init__.py
from .router import router  # re-export for easy import in main.py

__all__ = ["router"]--- ./network_snapshot/schemas.py ---
# app/network_snapshot/schemas.py

from uuid import UUID
from typing import List, Optional

from pydantic import BaseModel, Field


class LengthByCategory(BaseModel):
    label: str
    length_km: float


class AssetValueByCategory(BaseModel):
    label: str
    value: float


class UnitCostByCategory(BaseModel):
    label: str
    cost_per_km: float


class NetworkSnapshot(BaseModel):
    project_id: UUID
    upload_id: UUID

    # --- Core from segments sheet ---
    total_length_km: float
    total_segments: int
    total_roads: Optional[int] = None

    length_by_road_class: List[LengthByCategory] = Field(default_factory=list)
    length_by_surface_type: List[LengthByCategory] = Field(default_factory=list)

    # --- From network_length sheet (optional) ---
    total_network_length_km: Optional[float] = None
    length_by_network_type: List[LengthByCategory] = Field(default_factory=list)

    # --- From asset_value sheet (optional) ---
    total_asset_value: Optional[float] = None
    asset_value_by_category: List[AssetValueByCategory] = Field(default_factory=list)

    # --- From road_costs sheet (optional) ---
    unit_costs_by_surface: List[UnitCostByCategory] = Field(default_factory=list)--- ./network_snapshot/router.py ---
# app/network_snapshot/router.py

from uuid import UUID
from fastapi import APIRouter, Depends

from app.routers.projects import get_current_user_id
from .schemas import NetworkSnapshot
from .service import get_network_snapshot

router = APIRouter()


@router.get(
    "/{project_id}/network/snapshot",
    response_model=NetworkSnapshot,
    summary="Get network snapshot for latest master data upload",
)
def read_network_snapshot(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    """
    Returns high-level network metrics (total length, segments,
    breakdown by class and surface) computed from the latest
    master data upload for this project & user.
    """
    return get_network_snapshot(project_id=project_id, user_id=user_id)--- ./computation/__init__.py ---
# app/computation/__init__.py
from .router import router  # so main.py can `from app.computation import router`

__all__ = ["router"]--- ./computation/engine.py ---
# app/computation/engine.py

import pandas as pd
import numpy as np
from uuid import UUID
from app.scenarios.schemas import RonetParameters
from .schemas import SimulationOutput, YearlyResult

def run_ronet_simulation(
    project_id: UUID,
    scenario_id: UUID,
    segments_df: pd.DataFrame,
    costs_df: pd.DataFrame,
    iri_defaults_df: pd.DataFrame,
    params: RonetParameters
) -> SimulationOutput:
    
    # --- 1. PREPARE LOOKUPS ---
    # Convert CSV data into easy-to-use dictionaries
    
    # Thresholds: (Road Class, Surface) -> { good_max, poor_min }
    threshold_map = {} 
    if not iri_defaults_df.empty:
        for _, row in iri_defaults_df.iterrows():
            # Safely handle string conversion and casing
            r_class = str(row.get('road_class', '')).strip().lower()
            s_type = str(row.get('surface_type', '')).strip().lower()
            key = (r_class, s_type)
            
            threshold_map[key] = {
                'good': float(row.get('iri_good_max', 2.5)),
                'poor': float(row.get('iri_poor_min', 6.0))
            }

    # Costs: (Treatment, Surface) -> { cost, reset_iri }
    cost_map = {}
    if not costs_df.empty:
        for _, row in costs_df.iterrows():
            t_type = str(row.get('treatment', '')).strip().lower()
            s_type = str(row.get('surface_type', '')).strip().lower()
            cost_map[(t_type, s_type)] = {
                'cost': float(row.get('cost_per_km', 0)),
                'reset_iri': float(row.get('reset_to_iri', 2.0))
            }

    # --- 2. INITIALIZE NETWORK STATE ---
    # Work on a copy so we don't mutate original data
    current_segments = segments_df.copy()
    
    # Ensure numeric types and handle missing values
    current_segments['iri'] = pd.to_numeric(current_segments['iri'], errors='coerce').fillna(4.0)
    current_segments['length_km'] = pd.to_numeric(current_segments['length_km'], errors='coerce').fillna(1.0)
    current_segments['aadt'] = pd.to_numeric(current_segments.get('aadt', 0), errors='coerce').fillna(0)

    yearly_results = []
    
    # Deterioration Rates (Simplified for Prototype)
    DETERIORATION_PAVED = 0.12
    DETERIORATION_GRAVEL = 0.25

    # --- 3. SIMULATION LOOP (YEAR BY YEAR) ---
    # Loop exactly the number of years specified by the slider
    for year in range(1, params.analysis_duration + 1):
        
        year_total_cost_needed = 0
        year_total_cost_spent = 0
        
        # Track potential interventions
        interventions = [] # List of dicts

        # --- A. DETERMINE NEEDS (Unconstrained) ---
        for idx, seg in current_segments.iterrows():
            r_class = str(seg.get('road_class', '')).strip().lower()
            s_type = str(seg.get('surface_type', '')).strip().lower()
            iri = seg['iri']
            length = seg['length_km']
            
            # Get Thresholds (Default to 3.0/6.0 if not found)
            limits = threshold_map.get((r_class, s_type), {'good': 3.0, 'poor': 6.0})
            
            # 1. Deteriorate (Roads get older/rougher)
            det_rate = DETERIORATION_GRAVEL if 'gravel' in s_type else DETERIORATION_PAVED
            iri += det_rate
            
            # 2. Decision Logic (Based on Policy Bias)
            treatment_needed = None
            
            if params.policy_bias == "preventive":
                # Fix as soon as it leaves "Good" condition
                if iri > limits['good']: 
                    treatment_needed = "periodic reseal" if 'paved' in s_type else "regravelling"
            
            elif params.policy_bias == "reactive":
                # Fix only when it reaches "Poor" condition
                if iri > limits['poor']: 
                    treatment_needed = "rehabilitation" if 'paved' in s_type else "regravelling"
            
            else: # "balanced"
                if iri > limits['poor']:
                    treatment_needed = "rehabilitation" if 'paved' in s_type else "regravelling"
                elif iri > limits['good']:
                    # Only do lighter fix if we are strictly in Fair (and not Poor)
                    treatment_needed = "periodic reseal" if 'paved' in s_type else "blading"

            # 3. Calculate Cost if treatment is needed
            if treatment_needed:
                cost_info = cost_map.get((treatment_needed, s_type))
                
                # Fallback costs if CSV lookup fails
                if not cost_info:
                    if "reseal" in treatment_needed: cost_info = {'cost': 900000, 'reset_iri': 2.2}
                    elif "rehab" in treatment_needed: cost_info = {'cost': 3500000, 'reset_iri': 1.5}
                    elif "regravel" in treatment_needed: cost_info = {'cost': 600000, 'reset_iri': 4.0}
                    else: cost_info = {'cost': 50000, 'reset_iri': iri - 0.5}

                cost = cost_info['cost'] * length
                reset_iri = cost_info['reset_iri']
                
                year_total_cost_needed += cost
                interventions.append({
                    'idx': idx,
                    'cost': cost,
                    'reset_iri': reset_iri,
                    'current_iri': iri
                })
            
            # Temporarily save deteriorated state (will be overwritten if fixed)
            current_segments.at[idx, 'iri'] = iri

        # --- B. APPLY BUDGET CONSTRAINT ---
        # 100% means we spend exactly what is needed. <100% means we cut projects.
        budget_factor = params.budget_percent_baseline / 100.0
        available_budget = year_total_cost_needed * budget_factor
        
        # Sort interventions by AADT (High traffic gets priority funding)
        interventions.sort(key=lambda x: current_segments.at[x['idx'], 'aadt'], reverse=True)
        
        for action in interventions:
            if available_budget >= action['cost']:
                # Fund the project
                available_budget -= action['cost']
                year_total_cost_spent += action['cost']
                # Apply improvement (reset IRI)
                current_segments.at[action['idx'], 'iri'] = action['reset_iri']
            else:
                # No budget -> Road stays deteriorated
                pass

        # --- C. CALCULATE YEARLY STATS ---
        total_len = current_segments['length_km'].sum()
        if total_len == 0: total_len = 1 # Avoid division by zero
        
        # Weighted Average IRI
        avg_iri = (current_segments['iri'] * current_segments['length_km']).sum() / total_len
        
        # Calculate Condition Buckets
        good_km, fair_km, poor_km = 0, 0, 0
        
        for _, seg in current_segments.iterrows():
            r_class = str(seg.get('road_class', '')).strip().lower()
            s_type = str(seg.get('surface_type', '')).strip().lower()
            limits = threshold_map.get((r_class, s_type), {'good': 3.0, 'poor': 6.0})
            
            if seg['iri'] <= limits['good']: good_km += seg['length_km']
            elif seg['iri'] <= limits['poor']: fair_km += seg['length_km']
            else: poor_km += seg['length_km']

        yearly_results.append(YearlyResult(
            year=year,
            avg_condition_index=round(avg_iri, 2),
            pct_good=round((good_km / total_len) * 100, 1),
            pct_fair=round((fair_km / total_len) * 100, 1),
            pct_poor=round((poor_km / total_len) * 100, 1),
            total_maintenance_cost=round(year_total_cost_spent, 2),
            asset_value=0 # Placeholder for advanced asset value logic
        ))

    return SimulationOutput(
        project_id=str(project_id),
        scenario_id=str(scenario_id),
        year_count=params.analysis_duration,
        yearly_data=yearly_results,
        total_cost_npv=sum(y.total_maintenance_cost for y in yearly_results),
        final_network_condition=yearly_results[-1].avg_condition_index
    )--- ./computation/schemas.py ---
# app/computation/schemas.py

from typing import List, Dict, Optional
from pydantic import BaseModel

class YearlyResult(BaseModel):
    year: int
    avg_condition_index: float
    pct_good: float
    pct_fair: float
    pct_poor: float
    total_maintenance_cost: float
    asset_value: float

class SimulationOutput(BaseModel):
    project_id: str
    scenario_id: str
    year_count: int
    
    # Time Series Data (for charts)
    yearly_data: List[YearlyResult]
    
    # Aggregates (for summary cards)
    total_cost_npv: float
    final_network_condition: float--- ./computation/router.py ---
from fastapi import APIRouter, Depends, HTTPException
from uuid import UUID
import pandas as pd
from app.routers.projects import get_current_user_id, get_db_connection
from app.scenarios import service as scenario_service
from . import engine, schemas

router = APIRouter()

# --- ðŸ‘‡ THIS WAS MISSING: The Endpoint to fetch results ---
@router.get(
    "/{project_id}/simulation/latest",
    response_model=schemas.SimulationOutput,
    summary="Get results from the most recent simulation run"
)
def get_latest_simulation_result(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id)
):
    # Fetch the Baseline Scenario ID first
    scenario = scenario_service.get_or_create_baseline(project_id, user_id)
    
    # Get the result specifically for this scenario
    sql = """
        SELECT results_payload
        FROM public.simulation_results
        WHERE project_id = %s AND scenario_id = %s
        LIMIT 1;
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), str(scenario.id)))
            row = cur.fetchone()
            
    if not row:
        raise HTTPException(404, "No simulation results found for this project.")
        
    return row[0]


# --- Existing Run Logic (Unchanged) ---
@router.post(
    "/{project_id}/simulation/run",
    response_model=schemas.SimulationOutput,
    summary="Trigger a RONET simulation run"
)
def run_simulation(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id)
):
    # 1. Fetch Workbook Data (Segments + Lookups)
    try:
        sql = """
            SELECT workbook_payload 
            FROM public.master_data_uploads
            WHERE project_id = %s AND user_id = %s
            ORDER BY created_at DESC LIMIT 1;
        """
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (str(project_id), user_id))
                row = cur.fetchone()
                
        if not row or not row[0]:
            raise HTTPException(404, "No master data found. Please upload a workbook first.")
            
        payload = row[0]
        
        if "segments" not in payload:
             raise HTTPException(400, "Workbook missing 'segments' sheet.")
        df_segments = pd.DataFrame(payload["segments"])
        
        # Extract Lookups
        df_costs = pd.DataFrame(payload.get("road_costs", []))
        df_iri = pd.DataFrame(payload.get("iri_defaults", []))
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"Error loading master data: {e}")

    # 2. Fetch Scenario Parameters (The Sliders)
    # This fetches the 'analysis_duration' you saved in Step 2
    try:
        scenario = scenario_service.get_or_create_baseline(project_id, user_id)
    except Exception as e:
        raise HTTPException(500, f"Error loading scenario: {e}")

    # 3. Run Engine
    # The 'params' object contains the exact years set in your UI slider
    try:
        result = engine.run_ronet_simulation(
            project_id=project_id,
            scenario_id=scenario.id,
            segments_df=df_segments,
            costs_df=df_costs,
            iri_defaults_df=df_iri,
            params=scenario.parameters 
        )
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"Simulation engine failed: {e}")

    # 4. Save Results to DB (OVERWRITE MODE)
    # We delete any existing result for this specific scenario first
    sql_delete = """
        DELETE FROM public.simulation_results
        WHERE project_id = %s AND scenario_id = %s;
    """
    
    sql_insert = """
        INSERT INTO public.simulation_results 
        (project_id, scenario_id, results_payload, triggered_by)
        VALUES (%s, %s, %s, %s);
    """
    
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            # Step A: Clear old run
            cur.execute(sql_delete, (str(project_id), str(scenario.id)))
            
            # Step B: Insert new run
            cur.execute(sql_insert, (
                str(project_id), 
                str(scenario.id), 
                result.model_dump_json(), 
                user_id
            ))
            conn.commit()

    return result--- ./dashboards/service.py ---
# app/dashboards/service.py
from typing import List
from uuid import UUID

from fastapi import HTTPException

from .schemas import DashboardCreate, DashboardOut, DashboardUpdate
from . import repository


def list_dashboards_service(
    project_id: UUID,
    user_id: str,
) -> List[DashboardOut]:
    rows = repository.list_dashboards_for_project(project_id, user_id)
    return [DashboardOut(**r) for r in rows]


def get_dashboard_service(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str,
) -> DashboardOut:
    row = repository.fetch_dashboard(project_id, dashboard_id, user_id)
    if not row:
        raise HTTPException(status_code=404, detail="Dashboard not found.")
    return DashboardOut(**row)


def create_dashboard_service(
    project_id: UUID,
    user_id: str,
    data: DashboardCreate,
) -> DashboardOut:
    payload = data.model_dump()
    row = repository.insert_dashboard(project_id, user_id, payload)
    return DashboardOut(**row)


def update_dashboard_service(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str,
    data: DashboardUpdate,
) -> DashboardOut:
    payload = {k: v for k, v in data.model_dump().items() if v is not None}
    row = repository.update_dashboard(project_id, dashboard_id, user_id, payload)
    if not row:
        raise HTTPException(status_code=404, detail="Dashboard not found.")
    return DashboardOut(**row)--- ./dashboards/__init__.py ---
# app/dashboards/__init__.py
from .router import router  # so main.py can `from app.dashboards import router`

__all__ = ["router"]--- ./dashboards/schemas.py ---
# app/dashboards/schemas.py
from typing import Any, Dict, Optional
from uuid import UUID

from pydantic import BaseModel, Field


class DashboardBase(BaseModel):
    name: str = Field(..., max_length=200)
    description: Optional[str] = None
    is_favorite: bool = False
    layout: Optional[Dict[str, Any]] = None
    overrides: Optional[Dict[str, Any]] = None


class DashboardCreate(DashboardBase):
    """Payload for creating a dashboard."""
    pass


class DashboardUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    is_favorite: Optional[bool] = None
    layout: Optional[Dict[str, Any]] = None
    overrides: Optional[Dict[str, Any]] = None


class DashboardOut(DashboardBase):
    id: UUID
    project_id: UUID
    user_id: str
    created_at: str
    updated_at: str

    class Config:
        from_attributes = True--- ./dashboards/router.py ---
# app/dashboards/router.py
from typing import List
from uuid import UUID

from fastapi import APIRouter, Depends

from app.routers.projects import get_current_user_id
from .schemas import DashboardCreate, DashboardOut, DashboardUpdate
from . import service

router = APIRouter()


@router.get(
    "/{project_id}/dashboards",
    response_model=List[DashboardOut],
    summary="List dashboards for a project",
)
def list_dashboards(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.list_dashboards_service(project_id, user_id)


@router.post(
    "/{project_id}/dashboards",
    response_model=DashboardOut,
    summary="Create a new dashboard",
)
def create_dashboard(
    project_id: UUID,
    payload: DashboardCreate,
    user_id: str = Depends(get_current_user_id),
):
    return service.create_dashboard_service(project_id, user_id, payload)


@router.get(
    "/{project_id}/dashboards/{dashboard_id}",
    response_model=DashboardOut,
    summary="Get a single dashboard",
)
def get_dashboard(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.get_dashboard_service(project_id, dashboard_id, user_id)


@router.put(
    "/{project_id}/dashboards/{dashboard_id}",
    response_model=DashboardOut,
    summary="Update an existing dashboard",
)
def update_dashboard(
    project_id: UUID,
    dashboard_id: UUID,
    payload: DashboardUpdate,
    user_id: str = Depends(get_current_user_id),
):
    return service.update_dashboard_service(project_id, dashboard_id, user_id, payload)--- ./dashboards/repository.py ---
# app/dashboards/repository.py
from typing import Any, Dict, List, Optional
from uuid import UUID

from psycopg2.extras import Json

from app.routers.projects import get_db_connection


def list_dashboards_for_project(
    project_id: UUID,
    user_id: str,
) -> List[Dict[str, Any]]:
    sql = """
        SELECT
            id, project_id, user_id,
            name, description, is_favorite,
            layout, overrides,
            created_at, updated_at
        FROM public.project_dashboards
        WHERE project_id = %s AND user_id = %s
        ORDER BY is_favorite DESC, created_at DESC;
    """
    with get_db_connection() as conn, conn.cursor() as cur:
        cur.execute(sql, (str(project_id), user_id))
        rows = cur.fetchall()
        if not rows:
            return []
        cols = [d[0] for d in cur.description]
        return [dict(zip(cols, r)) for r in rows]


def fetch_dashboard(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str,
) -> Optional[Dict[str, Any]]:
    sql = """
        SELECT
            id, project_id, user_id,
            name, description, is_favorite,
            layout, overrides,
            created_at, updated_at
        FROM public.project_dashboards
        WHERE id = %s AND project_id = %s AND user_id = %s
        LIMIT 1;
    """
    with get_db_connection() as conn, conn.cursor() as cur:
        cur.execute(sql, (str(dashboard_id), str(project_id), user_id))
        row = cur.fetchone()
        if not row:
            return None
        cols = [d[0] for d in cur.description]
        return dict(zip(cols, row))


def insert_dashboard(
    project_id: UUID,
    user_id: str,
    payload: Dict[str, Any],
) -> Dict[str, Any]:
    sql = """
        INSERT INTO public.project_dashboards (
            project_id, user_id,
            name, description, is_favorite,
            layout, overrides
        )
        VALUES (%s,%s,%s,%s,%s,%s,%s)
        RETURNING
            id, project_id, user_id,
            name, description, is_favorite,
            layout, overrides,
            created_at, updated_at;
    """
    with get_db_connection() as conn, conn.cursor() as cur:
        cur.execute(
            sql,
            (
                str(project_id),
                user_id,
                payload.get("name"),
                payload.get("description"),
                payload.get("is_favorite", False),
                Json(payload.get("layout")) if payload.get("layout") is not None else None,
                Json(payload.get("overrides")) if payload.get("overrides") is not None else None,
            ),
        )
        row = cur.fetchone()
        cols = [d[0] for d in cur.description]
        conn.commit()
        return dict(zip(cols, row))


def update_dashboard(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str,
    payload: Dict[str, Any],
) -> Optional[Dict[str, Any]]:
    sql = """
        UPDATE public.project_dashboards
        SET
            name = COALESCE(%s, name),
            description = COALESCE(%s, description),
            is_favorite = COALESCE(%s, is_favorite),
            layout = COALESCE(%s, layout),
            overrides = COALESCE(%s, overrides),
            updated_at = now()
        WHERE id = %s AND project_id = %s AND user_id = %s
        RETURNING
            id, project_id, user_id,
            name, description, is_favorite,
            layout, overrides,
            created_at, updated_at;
    """
    with get_db_connection() as conn, conn.cursor() as cur:
        cur.execute(
            sql,
            (
                payload.get("name"),
                payload.get("description"),
                payload.get("is_favorite"),
                Json(payload.get("layout")) if "layout" in payload else None,
                Json(payload.get("overrides")) if "overrides" in payload else None,
                str(dashboard_id),
                str(project_id),
                user_id,
            ),
        )
        row = cur.fetchone()
        if not row:
            return None
        cols = [d[0] for d in cur.description]
        conn.commit()
        return dict(zip(cols, row))--- ./scenarios/service.py ---
# app/scenarios/service.py

from uuid import UUID
from typing import List
from fastapi import HTTPException

from .schemas import (
    ScenarioCreate,
    ScenarioUpdate,
    ScenarioRead,
    ScenarioSummary,
    RonetParameters
)
from . import repository as repo

def get_or_create_baseline(project_id: UUID, user_id: str) -> ScenarioRead:
    """
    Fetch the baseline scenario. If none exists, create a default one.
    This ensures the 'Config' page always has something to load.
    """
    row = repo.get_baseline_scenario(project_id, user_id)
    
    if row:
        return ScenarioRead(**row)
    
    # Create default baseline if missing
    # We explicitly initialize default parameters here
    default_payload = ScenarioCreate(
        name="Baseline Scenario",
        description="Default configuration generated by system.",
        is_baseline=True,
        parameters=RonetParameters() 
    )
    
    # Create it
    new_row = repo.create_scenario(
        project_id, 
        user_id, 
        default_payload.model_dump()
    )
    return ScenarioRead(**new_row)

def update_scenario(
    project_id: UUID,
    scenario_id: UUID,
    user_id: str,
    payload: ScenarioUpdate,
) -> ScenarioRead:
    # Exclude unset to allow partial updates (PATCH behavior)
    data = payload.model_dump(exclude_unset=True)
    
    row = repo.update_scenario(project_id, scenario_id, user_id, data)
    if not row:
        raise HTTPException(status_code=404, detail="Scenario not found.")
    return ScenarioRead(**row)

def list_scenarios(project_id: UUID, user_id: str) -> List[ScenarioSummary]:
    rows = repo.list_scenarios(project_id, user_id)
    return [ScenarioSummary(**row) for row in rows]

def get_scenario(project_id: UUID, scenario_id: UUID, user_id: str) -> ScenarioRead:
    row = repo.get_scenario(project_id, scenario_id, user_id)
    if not row:
        raise HTTPException(status_code=404, detail="Scenario not found.")
    return ScenarioRead(**row)--- ./scenarios/__init__.py ---
# app/scenarios/__init__.py
from .router import router

__all__ = ["router"]--- ./scenarios/schemas.py ---
# app/scenarios/schemas.py

from uuid import UUID
from typing import Optional, Literal
from datetime import datetime
from pydantic import BaseModel, Field

# --- The shape of the JSON stored in 'parameters' column ---
# This matches the sliders on your frontend
class RonetParameters(BaseModel):
    analysis_duration: int = Field(20, ge=5, le=30, description="Analysis period in years")
    budget_strategy: Literal["unconstrained", "fixed_limit", "percent_baseline"] = "percent_baseline"
    annual_budget_cap: Optional[float] = None
    budget_percent_baseline: int = Field(100, ge=50, le=150, description="% of required budget")
    policy_bias: Literal["preventive", "balanced", "reactive"] = "balanced"
    discount_rate: float = 8.0

# --- CRUD Schemas ---

class ScenarioCreate(BaseModel):
    name: str
    description: Optional[str] = None
    is_baseline: bool = False
    # Default to standard parameters if none provided
    parameters: RonetParameters = Field(default_factory=RonetParameters)

class ScenarioUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    # Allow partial updates to parameters
    parameters: Optional[RonetParameters] = None

class ScenarioRead(BaseModel):
    id: UUID
    project_id: UUID
    user_id: UUID
    name: str
    description: Optional[str]
    is_baseline: bool
    parameters: RonetParameters  # API always returns parsed structure
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True

class ScenarioSummary(BaseModel):
    id: UUID
    name: str
    is_baseline: bool
    created_at: datetime--- ./scenarios/router.py ---
# app/scenarios/router.py

from uuid import UUID
from typing import List
from fastapi import APIRouter, Depends, status

from app.routers.projects import get_current_user_id
from .schemas import (
    ScenarioCreate,
    ScenarioUpdate,
    ScenarioRead,
    ScenarioSummary,
)
from . import service

router = APIRouter()

# --- Config Page Endpoint ---
@router.get(
    "/{project_id}/scenarios/baseline",
    response_model=ScenarioRead,
    summary="Get (or create) the baseline scenario for config page"
)
def get_project_baseline(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.get_or_create_baseline(project_id, user_id)

# --- Standard CRUD ---

@router.post(
    "/{project_id}/scenarios",
    response_model=ScenarioRead,
    status_code=status.HTTP_201_CREATED,
)
def create_project_scenario(
    project_id: UUID,
    payload: ScenarioCreate,
    user_id: str = Depends(get_current_user_id),
):
    return service.create_scenario(project_id, user_id, payload)

@router.get(
    "/{project_id}/scenarios",
    response_model=List[ScenarioSummary],
)
def list_project_scenarios(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.list_scenarios(project_id, user_id)

@router.get(
    "/{project_id}/scenarios/{scenario_id}",
    response_model=ScenarioRead,
)
def get_project_scenario(
    project_id: UUID,
    scenario_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.get_scenario(project_id, scenario_id, user_id)

@router.put(
    "/{project_id}/scenarios/{scenario_id}",
    response_model=ScenarioRead,
)
def update_project_scenario(
    project_id: UUID,
    scenario_id: UUID,
    payload: ScenarioUpdate,
    user_id: str = Depends(get_current_user_id),
):
    return service.update_scenario(project_id, scenario_id, user_id, payload)--- ./scenarios/repository.py ---
# app/scenarios/repository.py

from uuid import UUID
from typing import List, Dict, Any, Optional
from psycopg2.extras import Json
from app.routers.projects import get_db_connection

def create_scenario(
    project_id: UUID,
    user_id: str,
    payload: Dict[str, Any],
) -> Dict[str, Any]:
    sql = """
        INSERT INTO public.project_scenarios (
            project_id, user_id,
            name, description, is_baseline, parameters
        )
        VALUES (%s, %s, %s, %s, %s, %s)
        RETURNING
            id, project_id, user_id,
            name, description, is_baseline,
            parameters, created_at, updated_at;
    """
    
    # Ensure parameters are serialized to JSON
    # payload["parameters"] comes in as a dict from the Pydantic model
    params_json = Json(payload.get("parameters", {}))

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(
                sql,
                (
                    str(project_id),
                    user_id,
                    payload["name"],
                    payload.get("description"),
                    payload.get("is_baseline", False),
                    params_json,
                ),
            )
            row = cur.fetchone()
            conn.commit()
            cols = [d[0] for d in cur.description]
            return dict(zip(cols, row))

def list_scenarios(project_id: UUID, user_id: str) -> List[Dict[str, Any]]:
    sql = """
        SELECT id, name, is_baseline, created_at
        FROM public.project_scenarios
        WHERE project_id = %s AND user_id = %s
        ORDER BY is_baseline DESC, created_at DESC;
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            rows = cur.fetchall()
            cols = [d[0] for d in cur.description]
            return [dict(zip(cols, r)) for r in rows]

def get_scenario(project_id: UUID, scenario_id: UUID, user_id: str) -> Optional[Dict[str, Any]]:
    sql = """
        SELECT * FROM public.project_scenarios
        WHERE project_id = %s AND id = %s AND user_id = %s;
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), str(scenario_id), user_id))
            row = cur.fetchone()
            if not row:
                return None
            cols = [d[0] for d in cur.description]
            return dict(zip(cols, row))

def get_baseline_scenario(project_id: UUID, user_id: str) -> Optional[Dict[str, Any]]:
    """Helper to find the designated baseline scenario."""
    sql = """
        SELECT * FROM public.project_scenarios
        WHERE project_id = %s AND user_id = %s AND is_baseline = true
        LIMIT 1;
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            row = cur.fetchone()
            if not row:
                return None
            cols = [d[0] for d in cur.description]
            return dict(zip(cols, row))

def update_scenario(
    project_id: UUID,
    scenario_id: UUID,
    user_id: str,
    payload: Dict[str, Any],
) -> Optional[Dict[str, Any]]:
    
    fields = []
    values = []

    if "name" in payload:
        fields.append("name = %s")
        values.append(payload["name"])
    if "description" in payload:
        fields.append("description = %s")
        values.append(payload["description"])
    if "is_baseline" in payload:
        fields.append("is_baseline = %s")
        values.append(payload["is_baseline"])
    if "parameters" in payload:
        fields.append("parameters = %s")
        # Ensure we wrap the dict in Json() adapter
        values.append(Json(payload["parameters"]))

    if not fields:
        return get_scenario(project_id, scenario_id, user_id)

    fields.append("updated_at = now()")
    
    sql = f"""
        UPDATE public.project_scenarios
        SET {", ".join(fields)}
        WHERE project_id = %s AND id = %s AND user_id = %s
        RETURNING *;
    """
    
    values.extend([str(project_id), str(scenario_id), user_id])

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, tuple(values))
            row = cur.fetchone()
            conn.commit()
            if not row:
                return None
            cols = [d[0] for d in cur.description]
            return dict(zip(cols, row))--- ./db/models.py ---
# app/db/models.py

from pydantic import BaseModel
from typing import Optional
from datetime import datetime
from uuid import UUID
import os



JWT_SECRET = os.getenv("SUPABASE_JWT_SECRET")
ALGORITHM = "HS256"

print("JWT_SECRET loaded? ->", bool(JWT_SECRET))  


# Schema used for retrieving a saved project from the database
class ProjectDB(BaseModel):
    id: UUID
    user_id: UUID
    project_name: str
    description: Optional[str] = None
    start_year: int
    forecast_duration: int
    discount_rate: float # Numeric in PG, float in Python
    created_at: datetime
    updated_at: datetime
    
    class Config:
        orm_mode = True # Allows Pydantic to read ORM objects--- ./db/__init__.py ---
--- ./db/schemas.py ---
# app/db/schemas.py

from pydantic import BaseModel
from typing import Optional, Dict, Any, List
from datetime import datetime
from uuid import UUID

# ============================================================
# PROJECT Schemas
# ============================================================

class ProjectMetadata(BaseModel):
    project_name: str
    description: Optional[str] = None
    start_year: int
    forecast_duration: int
    discount_rate: float


class ProjectDB(BaseModel):
    id: UUID
    user_id: UUID
    project_name: str
    description: Optional[str] = None
    start_year: int
    forecast_duration: int
    discount_rate: float
    created_at: datetime
    updated_at: datetime
    
    class Config:
        from_attributes = True

# ============================================================
# MASTER DATA Schemas
# ============================================================

class MasterDataUploadStatus(BaseModel):
    id: UUID
    project_id: UUID
    user_id: UUID
    original_filename: str
    mime_type: str
    file_size: int
    status: str
    row_count: Optional[int] = None
    validation_errors: Optional[Dict[str, Any]] = None
    created_at: datetime--- ./main.py ---
# app/main.py (FINAL UPDATED VERSION)

from pathlib import Path
from dotenv import load_dotenv

# -------------------------------------------------------------------
# Load .env BEFORE importing anything that relies on environment vars
# -------------------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent.parent
load_dotenv(BASE_DIR / ".env")

# -------------------------------------------------------------------
# FastAPI + CORS
# -------------------------------------------------------------------
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

# Routers
# NOTE: Ensure each module's __init__.py exposes 'router' 
# or change imports to: from app.master_data.router import router as ...
from app.routers import projects
from app.master_data import router as master_data_router
from app.network_snapshot import router as network_snapshot_router
from app.scenarios import router as scenarios_router
from app.computation import router as computation_router
# Only include this if the dashboards module exists to avoid startup errors
from app.dashboards import router as dashboards_router 
from app.routers import provincial_stats

# -------------------------------------------------------------------
# FastAPI APP CONFIG
# -------------------------------------------------------------------
app = FastAPI(
    title="Mosianedi Investment API",
    description="API Gateway for RONET computation and scenario management.",
)


# -------------------------------------------------------------------
# MIDDLEWARE
# -------------------------------------------------------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all â€” adjust for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# -------------------------------------------------------------------
# ROUTES
# -------------------------------------------------------------------
app.include_router(
    projects.router,
    prefix="/api/v1/projects",
    tags=["Projects"],
)

app.include_router(
    master_data_router,
    prefix="/api/v1/projects",
    tags=["Master Data"],
)

app.include_router(
    network_snapshot_router, 
    prefix="/api/v1/projects", 
    tags=["Network"]
)

app.include_router(
    scenarios_router, 
    prefix="/api/v1/projects", 
    tags=["Scenarios"]
)


app.include_router(
    computation_router,
    prefix="/api/v1/projects",
    tags=["Computation"]
)
# Uncomment when the dashboards module is ready
app.include_router(
    dashboards_router, 
    prefix="/api/v1/projects", 
    tags=["Dashboards"]
)

app.include_router(
    provincial_stats.router, 
    prefix="/api/v1/provincial-stats", 
    tags=["Provincial Stats"]
)

# -------------------------------------------------------------------
# ROOT PING / HEALTHCHECK
# -------------------------------------------------------------------
@app.get("/")
def read_root():
    return {
        "status": "ok",
        "service": "Mosianedi Investment API"
    }--- ./proposal_data/service.py ---
from typing import Dict, Any
from .schemas import ProposalDataPayload


def compute_total_km(p: ProposalDataPayload) -> float:
    return (
        p.km_arid
        + p.km_semi_arid
        + p.km_dry_sub_humid
        + p.km_moist_sub_humid
        + p.km_humid
    )


def to_inputs_payload(p: ProposalDataPayload) -> Dict[str, Any]:
    # Store as JSON in projects.inputs_payload for demo speed (no new tables needed)
    return {
        "km_arid": p.km_arid,
        "km_semi_arid": p.km_semi_arid,
        "km_dry_sub_humid": p.km_dry_sub_humid,
        "km_moist_sub_humid": p.km_moist_sub_humid,
        "km_humid": p.km_humid,
        "avg_vci": p.avg_vci,
        "vehicle_km": p.vehicle_km,
        "pct_vehicle_km_used": p.pct_vehicle_km_used,
        "fuel_sales": p.fuel_sales,
        "notes": p.notes,
        "extra_inputs": p.extra_inputs or {},
    }


def from_inputs_payload(d: Dict[str, Any]) -> ProposalDataPayload:
    # Defensive defaults so GET works even when nothing saved yet
    return ProposalDataPayload(
        km_arid=float(d.get("km_arid", 0) or 0),
        km_semi_arid=float(d.get("km_semi_arid", 0) or 0),
        km_dry_sub_humid=float(d.get("km_dry_sub_humid", 0) or 0),
        km_moist_sub_humid=float(d.get("km_moist_sub_humid", 0) or 0),
        km_humid=float(d.get("km_humid", 0) or 0),
        avg_vci=float(d.get("avg_vci", 0) or 0),
        vehicle_km=float(d.get("vehicle_km", 0) or 0),
        pct_vehicle_km_used=float(d.get("pct_vehicle_km_used", 0) or 0),
        fuel_sales=float(d.get("fuel_sales", 0) or 0),
        notes=d.get("notes"),
        extra_inputs=d.get("extra_inputs") or {},
    )
--- ./proposal_data/__init__.py ---
# app/master_data/__init__.py
from .router import router

__all__ = ["router"]--- ./proposal_data/schemas.py ---
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any


class ProposalDataPayload(BaseModel):
    # Climate zone km
    km_arid: float = Field(0, ge=0)
    km_semi_arid: float = Field(0, ge=0)
    km_dry_sub_humid: float = Field(0, ge=0)
    km_moist_sub_humid: float = Field(0, ge=0)
    km_humid: float = Field(0, ge=0)

    # Indicators
    avg_vci: float = Field(0, ge=0)
    vehicle_km: float = Field(0, ge=0)
    pct_vehicle_km_used: float = Field(0, ge=0, le=100)
    fuel_sales: float = Field(0, ge=0)

    notes: Optional[str] = None
    extra_inputs: Dict[str, Any] = {}


class ProposalDataResponse(BaseModel):
    project_id: str
    user_id: str
    total_km: float
    payload: ProposalDataPayload
    updated_at: Optional[str] = None
--- ./proposal_data/router.py ---
from fastapi import APIRouter, Depends, HTTPException
from app.routers.projects import get_db_connection  # uses your existing helper
from app.auth import get_current_user  # adjust import to your project
from .schemas import ProposalDataPayload, ProposalDataResponse
from . import repository
from .service import compute_total_km, to_inputs_payload, from_inputs_payload

router = APIRouter(prefix="/projects", tags=["proposal-data"])


@router.get("/{project_id}/proposal-data", response_model=ProposalDataResponse)
def read_proposal_data(project_id: str, user=Depends(get_current_user)):
    conn = get_db_connection()
    try:
        row = repository.get_proposal_data(conn, project_id, user["id"])
        if not row:
            raise HTTPException(status_code=404, detail="Project not found")

        payload = from_inputs_payload(row["inputs_payload"])
        return ProposalDataResponse(
            project_id=project_id,
            user_id=str(user["id"]),
            total_km=compute_total_km(payload),
            payload=payload,
            updated_at=row.get("updated_at"),
        )
    finally:
        conn.close()


@router.put("/{project_id}/proposal-data", response_model=ProposalDataResponse)
def save_proposal_data(project_id: str, body: ProposalDataPayload, user=Depends(get_current_user)):
    conn = get_db_connection()
    try:
        try:
            validate_or_raise(body)
        except ProposalValidationError as e:
            # 422 is standard for validation errors
            raise HTTPException(status_code=422, detail={"errors": e.errors})

        saved = repository.save_proposal_data(conn, project_id, user["id"], to_inputs_payload(body))
        payload = from_inputs_payload(saved["inputs_payload"])
        return ProposalDataResponse(
            project_id=project_id,
            user_id=str(user["id"]),
            total_km=compute_total_km(payload),
            payload=payload,
            updated_at=saved.get("updated_at"),
        )
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    finally:
        conn.close()
--- ./proposal_data/repository.py ---
from typing import Optional, Dict, Any
from datetime import datetime


def get_proposal_data(conn, project_id: str, user_id: str) -> Optional[Dict[str, Any]]:
    cur = conn.cursor()
    cur.execute(
        """
        SELECT inputs_payload, updated_at
        FROM public.projects
        WHERE id = %s AND user_id = %s
        """,
        (project_id, user_id),
    )
    row = cur.fetchone()
    cur.close()
    if not row:
        return None

    inputs_payload, updated_at = row
    return {
        "inputs_payload": inputs_payload or {},
        "updated_at": updated_at.isoformat() if isinstance(updated_at, datetime) else updated_at,
    }


def save_proposal_data(conn, project_id: str, user_id: str, inputs_payload: Dict[str, Any]) -> Dict[str, Any]:
    cur = conn.cursor()
    cur.execute(
        """
        UPDATE public.projects
        SET inputs_payload = %s,
            updated_at = now()
        WHERE id = %s AND user_id = %s
        RETURNING inputs_payload, updated_at
        """,
        (inputs_payload, project_id, user_id),
    )
    row = cur.fetchone()
    conn.commit()
    cur.close()

    if not row:
        raise ValueError("Project not found or not owned by user")

    payload, updated_at = row
    return {
        "inputs_payload": payload or {},
        "updated_at": updated_at.isoformat() if isinstance(updated_at, datetime) else updated_at,
    }
--- ./proposal_data/validation.py ---
from typing import Dict, Any, List, Tuple
from .schemas import ProposalDataPayload


class ProposalValidationError(Exception):
    def __init__(self, errors: List[Dict[str, Any]]):
        super().__init__("Proposal data validation failed")
        self.errors = errors


def validate_proposal_data(payload: ProposalDataPayload) -> Tuple[bool, List[Dict[str, Any]]]:
    """
    Returns (ok, errors). Each error is:
      { "field": "<field or group>", "message": "<human readable>", "code": "<stable_code>" }
    """
    errors: List[Dict[str, Any]] = []

    # Compute total km (business rule)
    total_km = (
        payload.km_arid
        + payload.km_semi_arid
        + payload.km_dry_sub_humid
        + payload.km_moist_sub_humid
        + payload.km_humid
    )

    if total_km <= 0:
        errors.append(
            {
                "field": "climate_km_total",
                "message": "Total climate kilometres must be greater than 0.",
                "code": "CLIMATE_TOTAL_ZERO",
            }
        )

    # Soft sanity checks (still allow save, but you can decide to treat as errors)
    # For demo simplicity, keep as errors only if obviously invalid.
    if payload.avg_vci == 0:
        errors.append(
            {
                "field": "avg_vci",
                "message": "Average VCI is 0. If unknown, enter an estimate rather than 0.",
                "code": "AVG_VCI_ZERO",
            }
        )

    # vehicle_km can be 0 in some edge cases, but usually indicates missing data
    if payload.vehicle_km == 0:
        errors.append(
            {
                "field": "vehicle_km",
                "message": "Vehicle-km is 0. If unknown, enter an estimate rather than 0.",
                "code": "VEHICLE_KM_ZERO",
            }
        )

    ok = len(errors) == 0
    return ok, errors


def validate_or_raise(payload: ProposalDataPayload) -> None:
    ok, errors = validate_proposal_data(payload)
    if not ok:
        raise ProposalValidationError(errors)
