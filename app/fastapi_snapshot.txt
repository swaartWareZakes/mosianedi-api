--- ./routers/provincial_stats.py ---
from fastapi import APIRouter, HTTPException, Depends, UploadFile, File
from pydantic import BaseModel
from typing import List, Optional
from uuid import UUID
import pandas as pd
import io
from app.routers.projects import get_db_connection, get_current_user_id

router = APIRouter()

# --- SCHEMAS --------------------------------------------------------
class ProvincialStatUpdate(BaseModel):
    province_name: str
    km_arid: Optional[float] = 0
    km_semi_arid: Optional[float] = 0
    km_dry_sub_humid: Optional[float] = 0
    km_moist_sub_humid: Optional[float] = 0
    km_humid: Optional[float] = 0
    avg_vci: Optional[float] = 0
    vehicle_km: Optional[float] = 0
    fuel_sales: Optional[float] = 0

class ProvincialStatResponse(ProvincialStatUpdate):
    id: UUID
    project_id: UUID

# --- ENDPOINTS ------------------------------------------------------

@router.get("/{project_id}", response_model=List[ProvincialStatResponse])
def get_stats(project_id: UUID, user_id: str = Depends(get_current_user_id)):
    """
    Returns the 9 rows. They are guaranteed to exist because of the SQL trigger.
    """
    sql = """
        SELECT * FROM public.provincial_stats
        WHERE project_id = %s
        ORDER BY province_name ASC;
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id),))
            rows = cur.fetchall()
            cols = [desc[0] for desc in cur.description]
            return [dict(zip(cols, row)) for row in rows]

@router.post("/{project_id}", status_code=200)
def save_manual_input(
    project_id: UUID, 
    stats: List[ProvincialStatUpdate], 
    user_id: str = Depends(get_current_user_id)
):
    """
    Saves user edits from the grid.
    Uses 'UPDATE' because the rows already exist.
    """
    sql = """
        UPDATE public.provincial_stats
        SET 
            km_arid = %s, km_semi_arid = %s, km_dry_sub_humid = %s, 
            km_moist_sub_humid = %s, km_humid = %s,
            avg_vci = %s, vehicle_km = %s, fuel_sales = %s,
            updated_at = NOW()
        WHERE project_id = %s AND province_name = %s;
    """
    
    values = []
    for s in stats:
        values.append((
            s.km_arid, s.km_semi_arid, s.km_dry_sub_humid, 
            s.km_moist_sub_humid, s.km_humid,
            s.avg_vci, s.vehicle_km, s.fuel_sales,
            str(project_id), s.province_name
        ))

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            # Execute batch update
            cur.executemany(sql, values)
            conn.commit()
            
    return {"message": "Saved successfully"}

@router.post("/{project_id}/upload")
def upload_stats_csv(
    project_id: UUID, 
    file: UploadFile = File(...), 
    user_id: str = Depends(get_current_user_id)
):
    """
    Parses 'Book1.csv' and updates the existing 9 rows.
    """
    try:
        contents = file.file.read()
        if file.filename.endswith('.csv'):
            df = pd.read_csv(io.BytesIO(contents), header=None)
        else:
            df = pd.read_excel(io.BytesIO(contents), header=None)

        # Basic Cleanup tailored to your CSV structure
        df = df.iloc[3:] # Skip headers
        df = df[df[0].notna()] # Remove empty lines
        
        updates = []
        for _, row in df.iterrows():
            prov_name = str(row[0]).strip()
            # Skip totals/junk
            if "Total" in prov_name or "Province" in prov_name:
                continue

            def parse(val):
                if isinstance(val, (int, float)): return val
                if isinstance(val, str):
                    clean = val.replace('R','').replace(',','').replace(' ','').replace('%','')
                    try: return float(clean)
                    except: return 0
                return 0

            # Map CSV Columns to DB Columns
            updates.append((
                parse(row[1]), parse(row[2]), parse(row[3]), parse(row[4]), parse(row[5]), # Climate Kms
                parse(row[7]), parse(row[8]), parse(row[10]), # VCI, VehKm, Fuel
                str(project_id), prov_name # WHERE clause
            ))

        sql = """
            UPDATE public.provincial_stats
            SET 
                km_arid=%s, km_semi_arid=%s, km_dry_sub_humid=%s, km_moist_sub_humid=%s, km_humid=%s,
                avg_vci=%s, vehicle_km=%s, fuel_sales=%s, updated_at=NOW()
            WHERE project_id=%s AND province_name=%s;
        """

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.executemany(sql, updates)
                conn.commit()

        return {"message": "Spreadsheet data imported"}

    except Exception as e:
        raise HTTPException(400, f"Upload Error: {str(e)}")--- ./routers/__init__.py ---
--- ./routers/master_data.py ---
"""
Master Data Upload, Validation, History & Preview Endpoints
===========================================================

Clean, modular, production-ready version.
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from typing import Optional, Dict, Any, List
from uuid import UUID
from io import BytesIO

import pandas as pd
import psycopg2
from psycopg2.extras import Json

from pydantic import BaseModel

# Shared helpers
from app.routers.projects import get_db_connection, get_current_user_id
from app.db.schemas import MasterDataUploadStatus


# ============================================================
# PREVIEW SCHEMAS
# ============================================================

class DataPreviewResponse(BaseModel):
    preview_data: List[Dict[str, Any]]
    total_rows: int
    columns: List[str]


# ============================================================
# REQUIRED COLUMNS FOR MASTER DATA
# ============================================================

REQUIRED_COLUMNS = {
    "segment_id",
    "road_id",
    "road_class",
    "length_km",
    "surface_type",
}

router = APIRouter()


# ============================================================
# HELPERS
# ============================================================

def _parse_master_data_file(file_bytes: bytes, filename: str) -> pd.DataFrame:
    """
    Convert uploaded Excel/CSV file → pandas DataFrame.
    """
    buffer = BytesIO(file_bytes)
    name = filename.lower()

    try:
        if name.endswith((".xlsx", ".xls")):
            return pd.read_excel(buffer)
        elif name.endswith(".csv"):
            return pd.read_csv(buffer)
        else:
            raise HTTPException(
                status_code=400,
                detail="Invalid file type. Use .xlsx, .xls or .csv."
            )
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail=f"Could not parse file: {str(e)}"
        )


# ============================================================
# UPLOAD ENDPOINT
# ============================================================

@router.post("/{project_id}/master-data/upload", response_model=MasterDataUploadStatus)
async def upload_master_data(
    project_id: UUID,
    file: UploadFile = File(...),
    user_id: str = Depends(get_current_user_id),
):
    """
    Upload + validate a master data file.
    Saves the file blob inline in PostgreSQL.
    """
    file_bytes = await file.read()
    if not file_bytes:
        raise HTTPException(status_code=400, detail="Uploaded file is empty.")

    mime_type = file.content_type or "application/octet-stream"
    file_size = len(file_bytes)

    validation_errors: Dict[str, Any] = {}
    status = "validated"
    row_count: Optional[int] = None

    # ---- VALIDATION ----
    try:
        df = _parse_master_data_file(file_bytes, file.filename)
        row_count = len(df.index)

        df_columns_lower = {c.lower(): c for c in df.columns}
        missing = [col for col in REQUIRED_COLUMNS if col not in df_columns_lower]

        if missing:
            status = "failed"
            validation_errors["missing_columns"] = missing

    except HTTPException:
        status = "failed"
        raise
    except Exception as e:
        status = "failed"
        validation_errors["validation_error"] = f"Unexpected: {str(e)}"

    # ---- INSERT INTO DB ----
    sql = """
        INSERT INTO public.master_data_uploads (
            project_id, user_id,
            original_filename, mime_type, file_size,
            storage_strategy, file_blob,
            status, row_count, validation_errors
        )
        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
        RETURNING
            id, project_id, user_id, original_filename, mime_type,
            file_size, status, row_count, validation_errors, created_at;
    """

    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (
                    str(project_id),
                    user_id,
                    file.filename,
                    mime_type,
                    file_size,
                    "inline",
                    psycopg2.Binary(file_bytes),
                    status,
                    row_count,
                    Json(validation_errors) if validation_errors else None
                ))

                record = dict(zip([d[0] for d in cur.description], cur.fetchone()))
                conn.commit()
                return record

    except Exception as e:
        print("[MASTER_DATA] Upload DB error:", e)
        raise HTTPException(500, "Internal server error during upload.")


# ============================================================
# LAST UPLOAD ENDPOINT
# ============================================================

@router.get("/{project_id}/master-data/last-upload", response_model=MasterDataUploadStatus)
async def get_last_master_data_upload(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id)
):
    """
    Return metadata for the most recent upload.
    """
    sql = """
        SELECT
            id, project_id, user_id,
            original_filename, mime_type, file_size,
            status, row_count, validation_errors, created_at
        FROM public.master_data_uploads
        WHERE project_id = %s AND user_id = %s
        ORDER BY created_at DESC
        LIMIT 1;
    """

    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (str(project_id), user_id))
                row = cur.fetchone()

                if not row:
                    raise HTTPException(404, "No master data uploads found.")

                record = dict(zip([d[0] for d in cur.description], row))
                return record

    except Exception as e:
        print("[MASTER_DATA] Last upload fetch error:", e)
        raise HTTPException(500, "Internal server error fetching last upload.")


# ============================================================
# PREVIEW ENDPOINT (LATEST FILE)
# ============================================================

@router.get("/{project_id}/master-data/preview", response_model=DataPreviewResponse)
async def get_master_data_preview(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id)
):
    """
    Reads the file_blobs of the most recent upload and returns 
    up to 50 preview rows.
    """
    sql = """
        SELECT file_blob, original_filename
        FROM public.master_data_uploads
        WHERE project_id = %s AND user_id = %s
        ORDER BY created_at DESC
        LIMIT 1;
    """

    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (str(project_id), user_id))
                row = cur.fetchone()

                if not row:
                    raise HTTPException(404, "No file found for preview.")

                file_blob_raw, filename = row

                if isinstance(file_blob_raw, memoryview):
                    file_bytes = file_blob_raw.tobytes()
                else:
                    file_bytes = file_blob_raw

    except Exception as e:
        print("[PREVIEW] DB error:", e)
        raise HTTPException(500, "Database error fetching file for preview.")

    # ---- PARSE ----
    try:
        df = _parse_master_data_file(file_bytes, filename)
        total_rows = len(df.index)

        preview_df = df.head(50).fillna("")
        preview_data = preview_df.to_dict("records")

        return DataPreviewResponse(
            preview_data=preview_data,
            total_rows=total_rows,
            columns=list(df.columns)
        )
    except Exception as e:
        print("[PREVIEW] Pandas error:", e)
        raise HTTPException(500, f"Error processing preview: {str(e)}")--- ./routers/projects.py ---
from fastapi import APIRouter, HTTPException, Depends, Header
import os
import psycopg2
from contextlib import contextmanager
from jose import jwt
from uuid import UUID
from typing import List, Dict, Any

from app.db.schemas import ProjectMetadata, ProjectDB

router = APIRouter()

JWT_SECRET = os.getenv("SUPABASE_JWT_SECRET")
ALGORITHM = "HS256"


@contextmanager
def get_db_connection():
    DB_URL = os.getenv("DATABASE_URL")
    if not DB_URL:
        raise ValueError("DATABASE_URL missing")

    conn = None
    try:
        conn = psycopg2.connect(DB_URL)
        yield conn
    finally:
        if conn:
            conn.close()


def get_current_user_id(authorization: str = Header(None)) -> str:
    if not authorization:
        raise HTTPException(401, "Authorization header missing")
    try:
        scheme, token = authorization.split()
        if scheme.lower() != "bearer":
            raise HTTPException(401, "Invalid auth scheme (expected Bearer)")

        payload = jwt.decode(
            token,
            JWT_SECRET,
            algorithms=[ALGORITHM],
            options={"verify_aud": False},
        )
        sub = payload.get("sub")
        if not sub:
            raise HTTPException(401, "Invalid token (sub missing)")
        return sub

    except HTTPException:
        raise
    except Exception:
        raise HTTPException(401, "Invalid token")


def _row_to_dict(cur, row) -> Dict[str, Any]:
    cols = [desc[0] for desc in cur.description]
    return dict(zip(cols, row))


# -------------------------------------------------------------------
# CREATE PROJECT (Proposal container) + initialize proposal_data row
# -------------------------------------------------------------------
@router.post("/", status_code=201)
async def create_project(
    metadata: ProjectMetadata,
    user_id: str = Depends(get_current_user_id),
):
    """
    Creates a project (proposal container) and immediately creates an empty proposal_data row (Option A).
    """

    sql_project = """
        INSERT INTO public.projects (user_id, project_name, province, start_year)
        VALUES (%s, %s, %s, %s)
        RETURNING id, created_at;
    """

    sql_proposal = """
        INSERT INTO public.proposal_data (project_id, user_id, data_source)
        VALUES (%s, %s, 'manual')
        ON CONFLICT (project_id) DO NOTHING;
    """

    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur:
                cur.execute(
                    sql_project,
                    (user_id, metadata.project_name, metadata.province, metadata.start_year),
                )
                project_id, created_at = cur.fetchone()

                cur.execute(sql_proposal, (str(project_id), user_id))

            conn.commit()

        except Exception as e:
            conn.rollback()
            raise HTTPException(status_code=500, detail=f"Failed to create project: {str(e)}")

    return {
        "project_id": str(project_id),
        "message": "Project created (proposal_data initialized)",
        "created_at": created_at.isoformat(),
    }


# -------------------------------------------------------------------
# GET ONE PROJECT
# -------------------------------------------------------------------
@router.get("/{project_id}", response_model=ProjectDB)
def get_project(project_id: UUID, user_id: str = Depends(get_current_user_id)):
    sql = """
        SELECT
          id,
          user_id,
          project_name,
          province,
          start_year,
          proposal_title,
          proposal_status,
          created_at,
          updated_at
        FROM public.projects
        WHERE id = %s AND user_id = %s;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            row = cur.fetchone()

            if not row:
                raise HTTPException(status_code=404, detail="Project not found")

            return _row_to_dict(cur, row)


# -------------------------------------------------------------------
# LIST PROJECTS
# -------------------------------------------------------------------
@router.get("/", response_model=List[ProjectDB])
async def list_projects(user_id: str = Depends(get_current_user_id)):
    sql = """
        SELECT
          id,
          user_id,
          project_name,
          province,
          start_year,
          proposal_title,
          proposal_status,
          created_at,
          updated_at
        FROM public.projects
        WHERE user_id = %s
        ORDER BY created_at DESC;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (user_id,))
            rows = cur.fetchall()
            return [_row_to_dict(cur, r) for r in rows]
        
        --- ./__init__.py ---
--- ./network_snapshot/service.py ---
from uuid import UUID
from typing import Dict, Any
from app.routers.projects import get_db_connection

def _n(x) -> float:
    """Helper to convert None to 0.0"""
    return float(x or 0)

def get_network_snapshot(project_id: UUID, user_id: str) -> Dict[str, Any]:
    # 1. Fetch Proposal Inputs
    sql = """
        SELECT 
            paved_arid, paved_semi_arid, paved_dry_sub_humid, paved_moist_sub_humid, paved_humid,
            gravel_arid, gravel_semi_arid, gravel_dry_sub_humid, gravel_moist_sub_humid, gravel_humid,
            avg_vci_used, vehicle_km, fuel_sales
        FROM public.proposal_data
        WHERE project_id = %s AND user_id = %s
    """
    
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            row = cur.fetchone()
            
            # If no data found, return zeros
            if not row:
                return {
                    "totalLengthKm": 0, "pavedLengthKm": 0, "gravelLengthKm": 0,
                    "avgVci": 0, "assetValue": 0, "totalVehicleKm": 0, "fuelSales": 0
                }
                
            data = dict(zip([d[0] for d in cur.description], row))

    # 2. Sum up lengths
    paved_total = (
        _n(data.get("paved_arid")) + _n(data.get("paved_semi_arid")) + 
        _n(data.get("paved_dry_sub_humid")) + _n(data.get("paved_moist_sub_humid")) + _n(data.get("paved_humid"))
    )
    
    gravel_total = (
        _n(data.get("gravel_arid")) + _n(data.get("gravel_semi_arid")) + 
        _n(data.get("gravel_dry_sub_humid")) + _n(data.get("gravel_moist_sub_humid")) + _n(data.get("gravel_humid"))
    )
    
    total_km = paved_total + gravel_total

    # 3. Calculate Asset Value (CRC)
    # Using standard engineering estimates (adjust as needed)
    RATE_PAVED = 3_500_000   # R3.5m per km
    RATE_GRAVEL = 250_000    # R250k per km
    
    asset_value = (paved_total * RATE_PAVED) + (gravel_total * RATE_GRAVEL)

    # 4. Return Flat Structure (matching React state)
    return {
        "totalLengthKm": round(total_km, 2),
        "pavedLengthKm": round(paved_total, 2),
        "gravelLengthKm": round(gravel_total, 2),
        "avgVci": _n(data.get("avg_vci_used")),
        "assetValue": round(asset_value, 2),
        "totalVehicleKm": _n(data.get("vehicle_km")),
        "fuelSales": _n(data.get("fuel_sales"))
    }--- ./network_snapshot/__init__.py ---
# app/network_snapshot/__init__.py
from .router import router  # re-export for easy import in main.py

__all__ = ["router"]--- ./network_snapshot/schemas.py ---
from pydantic import BaseModel
from typing import Optional

class NetworkProfileOut(BaseModel):
    # The essential stats for the card
    totalLengthKm: float
    pavedLengthKm: float
    gravelLengthKm: float
    
    avgVci: float
    assetValue: float      # The big money number (CRC)
    
    totalVehicleKm: float
    fuelSales: float
    
    # Optional metadata if needed later
    generated_at: Optional[str] = None--- ./network_snapshot/router.py ---
from uuid import UUID
from fastapi import APIRouter, Depends

from app.routers.projects import get_current_user_id
from .service import get_network_snapshot
from .schemas import NetworkProfileOut

router = APIRouter()

@router.get(
    "/{project_id}/network/snapshot",
    response_model=NetworkProfileOut,
    summary="Get calculated asset profile (CRC & VCI)",
)
def read_network_snapshot(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return get_network_snapshot(project_id=project_id, user_id=user_id)--- ./computation/__init__.py ---
# app/computation/__init__.py
from .router import router  # so main.py can `from app.computation import router`

__all__ = ["router"]--- ./computation/engine.py ---
from __future__ import annotations

from datetime import datetime, timezone
from uuid import UUID

from app.scenarios.schemas import ForecastParametersOut
from .schemas import SimulationOutput, YearlyResult, SimulationRunOptions


def run_ronet_simulation(
    project_id: UUID,
    params: ForecastParametersOut,
    network_profile: dict,
    options: SimulationRunOptions,
) -> SimulationOutput:
    """
    Lightweight RoNET-style simulation.

    NOTE: This is a simplified engine (demo-ready). We can harden the economics
    (true NPV with discount rate, deferred maintenance penalty, etc.) next.
    """

    yearly_results = []

    # 1) Determine scope
    paved_km = network_profile.get("pavedLengthKm", 0) if options.include_paved else 0
    gravel_km = network_profile.get("gravelLengthKm", 0) if options.include_gravel else 0

    # 2) Time setup
    duration = int(getattr(params, "analysis_duration", 5) or 5)
    start_year = options.start_year_override or (datetime.now(timezone.utc).year + 1)

    # 3) Financials
    inflation = float(getattr(params, "cpi_percentage", 6.0) or 0) / 100.0

    # --- engineering logic (placeholders but stable) ---
    base_need_paved = float(paved_km) * 160_000
    base_need_gravel = float(gravel_km) * 45_000
    total_annual_need_today = base_need_paved + base_need_gravel

    current_vci = float(network_profile.get("avgVci", 50) or 0)
    current_asset_value = float(network_profile.get("assetValue", 0) or 0)

    for i in range(duration):
        year = start_year + i

        # A) Apply inflation to annual need
        year_inflation_factor = (1 + inflation) ** i
        nominal_cost_needed = total_annual_need_today * year_inflation_factor

        # B) Simulate condition change (simple: small improvement when included)
        if (paved_km + gravel_km) > 0:
            improvement = 0.8
            current_vci = min(100.0, current_vci + improvement)
            current_asset_value *= (1 + (inflation * 0.5))
        else:
            current_vci = max(0.0, current_vci - 2.5)

        yearly_results.append(
            YearlyResult(
                year=year,
                avg_condition_index=round(current_vci, 2),
                pct_good=round(current_vci * 0.45, 1),
                pct_fair=round(current_vci * 0.35, 1),
                pct_poor=round(current_vci * 0.20, 1),
                total_maintenance_cost=round(nominal_cost_needed, 2),
                asset_value=round(current_asset_value, 2),
            )
        )

    total_cost = sum(y.total_maintenance_cost for y in yearly_results) if yearly_results else 0.0
    final_vci = yearly_results[-1].avg_condition_index if yearly_results else 0.0

    return SimulationOutput(
        project_id=str(project_id),
        year_count=duration,
        yearly_data=yearly_results,
        total_cost_npv=float(total_cost),
        final_network_condition=float(final_vci),
        generated_at=datetime.now(timezone.utc),
    )--- ./computation/schemas.py ---
from __future__ import annotations

from datetime import datetime, timezone
from typing import List, Optional
from uuid import UUID

from pydantic import BaseModel, Field


# ============================================================
# INPUT: Simulation run options (from frontend)
# ============================================================

class SimulationRunOptions(BaseModel):
    """
    Frontend sends camelCase, backend stores/uses snake_case.

    We keep aliases so BOTH of these work:
      - {"startYearOverride": 2027}
      - {"start_year_override": 2027}
    """
    start_year_override: Optional[int] = Field(None, alias="startYearOverride")
    include_paved: bool = Field(True, alias="includePaved")
    include_gravel: bool = Field(True, alias="includeGravel")

    class Config:
        populate_by_name = True


# ============================================================
# OUTPUT: Core simulation payload (stored in simulation_results.results_payload)
# ============================================================

class YearlyResult(BaseModel):
    year: int
    avg_condition_index: float
    pct_good: float
    pct_fair: float
    pct_poor: float
    total_maintenance_cost: float
    asset_value: float


class SimulationOutput(BaseModel):
    project_id: str
    year_count: int
    yearly_data: List[YearlyResult]
    total_cost_npv: float
    final_network_condition: float
    generated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))


# ============================================================
# OUTPUT: DB row wrapper (for history / audit / "who ran it")
# ============================================================

class SimulationRunOut(BaseModel):
    id: UUID
    project_id: UUID
    scenario_id: Optional[UUID] = None
    run_at: datetime
    triggered_by: Optional[UUID] = None
    status: str = "completed"
    results_payload: SimulationOutput

    class Config:
        from_attributes = True--- ./computation/router.py ---
from __future__ import annotations

from uuid import UUID
from typing import List

from fastapi import APIRouter, Depends, HTTPException, Query
from psycopg2.extras import Json

from app.routers.projects import get_current_user_id, get_db_connection
from app.scenarios import service as scenario_service
from . import engine, schemas

router = APIRouter()


@router.post(
    "/{project_id}/simulation/run",
    response_model=schemas.SimulationRunOut,
    summary="Trigger a Simulation and SAVE results (history kept)"
)
def run_simulation(
    project_id: UUID,
    options: schemas.SimulationRunOptions,
    user_id: str = Depends(get_current_user_id),
):
    # 1) Load prerequisites
    try:
        scenario_params = scenario_service.get_forecast(project_id, user_id)
        from app.network_snapshot.service import get_network_snapshot  # local import avoids circulars
        network_profile = get_network_snapshot(project_id, user_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading prerequisites: {e}")

    # 2) Run engine
    try:
        result = engine.run_ronet_simulation(
            project_id=project_id,
            params=scenario_params,
            network_profile=network_profile,
            options=options,
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Simulation engine failed: {e}")

    # 3) Save run to DB (DO NOT delete previous runs — we want history/audit)
    sql_insert = """
        INSERT INTO public.simulation_results
            (project_id, scenario_id, results_payload, triggered_by, status)
        VALUES
            (%s, %s, %s, %s, 'completed')
        RETURNING
            id, project_id, scenario_id, results_payload, run_at, triggered_by, status;
    """

    scenario_id = str(scenario_params.id) if getattr(scenario_params, "id", None) else None

    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur:
                cur.execute(
                    sql_insert,
                    (
                        str(project_id),
                        scenario_id,
                        Json(result.model_dump()),  # jsonb expects dict, not string
                        user_id,
                    ),
                )
                row = cur.fetchone()
                conn.commit()

                cols = [d[0] for d in cur.description]
                record = dict(zip(cols, row))
                return record

        except Exception as e:
            conn.rollback()
            raise HTTPException(status_code=500, detail=f"Failed to save simulation result: {e}")


@router.get(
    "/{project_id}/simulation/latest",
    response_model=schemas.SimulationRunOut,
    summary="Get the most recent simulation result (with audit fields)"
)
def get_latest_simulation(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    sql = """
        SELECT id, project_id, scenario_id, results_payload, run_at, triggered_by, status
        FROM public.simulation_results
        WHERE project_id = %s
        ORDER BY run_at DESC
        LIMIT 1;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id),))
            row = cur.fetchone()

            if not row:
                raise HTTPException(status_code=404, detail="No simulation results found.")

            cols = [d[0] for d in cur.description]
            return dict(zip(cols, row))


@router.get(
    "/{project_id}/simulation/history",
    response_model=List[schemas.SimulationRunOut],
    summary="List simulation runs for a project (history)"
)
def list_simulation_history(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
    limit: int = Query(20, ge=1, le=200),
):
    sql = """
        SELECT id, project_id, scenario_id, results_payload, run_at, triggered_by, status
        FROM public.simulation_results
        WHERE project_id = %s
        ORDER BY run_at DESC
        LIMIT %s;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), limit))
            rows = cur.fetchall()

            if not rows:
                return []

            cols = [d[0] for d in cur.description]
            return [dict(zip(cols, r)) for r in rows]--- ./dashboards/service.py ---
# app/dashboards/service.py
from typing import List
from uuid import UUID

from fastapi import HTTPException

from .schemas import DashboardCreate, DashboardOut, DashboardUpdate
from . import repository


def list_dashboards_service(
    project_id: UUID,
    user_id: str,
) -> List[DashboardOut]:
    rows = repository.list_dashboards_for_project(project_id, user_id)
    return [DashboardOut(**r) for r in rows]


def get_dashboard_service(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str,
) -> DashboardOut:
    row = repository.fetch_dashboard(project_id, dashboard_id, user_id)
    if not row:
        raise HTTPException(status_code=404, detail="Dashboard not found.")
    return DashboardOut(**row)


def create_dashboard_service(
    project_id: UUID,
    user_id: str,
    data: DashboardCreate,
) -> DashboardOut:
    payload = data.model_dump()
    row = repository.insert_dashboard(project_id, user_id, payload)
    return DashboardOut(**row)


def update_dashboard_service(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str,
    data: DashboardUpdate,
) -> DashboardOut:
    payload = {k: v for k, v in data.model_dump().items() if v is not None}
    row = repository.update_dashboard(project_id, dashboard_id, user_id, payload)
    if not row:
        raise HTTPException(status_code=404, detail="Dashboard not found.")
    return DashboardOut(**row)--- ./dashboards/__init__.py ---
# app/dashboards/__init__.py
from .router import router  # so main.py can `from app.dashboards import router`

__all__ = ["router"]--- ./dashboards/schemas.py ---
# app/dashboards/schemas.py
from typing import Any, Dict, Optional
from uuid import UUID

from pydantic import BaseModel, Field


class DashboardBase(BaseModel):
    name: str = Field(..., max_length=200)
    description: Optional[str] = None
    is_favorite: bool = False
    layout: Optional[Dict[str, Any]] = None
    overrides: Optional[Dict[str, Any]] = None


class DashboardCreate(DashboardBase):
    """Payload for creating a dashboard."""
    pass


class DashboardUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    is_favorite: Optional[bool] = None
    layout: Optional[Dict[str, Any]] = None
    overrides: Optional[Dict[str, Any]] = None


class DashboardOut(DashboardBase):
    id: UUID
    project_id: UUID
    user_id: str
    created_at: str
    updated_at: str

    class Config:
        from_attributes = True--- ./dashboards/router.py ---
# app/dashboards/router.py
from typing import List
from uuid import UUID

from fastapi import APIRouter, Depends

from app.routers.projects import get_current_user_id
from .schemas import DashboardCreate, DashboardOut, DashboardUpdate
from . import service

router = APIRouter()


@router.get(
    "/{project_id}/dashboards",
    response_model=List[DashboardOut],
    summary="List dashboards for a project",
)
def list_dashboards(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.list_dashboards_service(project_id, user_id)


@router.post(
    "/{project_id}/dashboards",
    response_model=DashboardOut,
    summary="Create a new dashboard",
)
def create_dashboard(
    project_id: UUID,
    payload: DashboardCreate,
    user_id: str = Depends(get_current_user_id),
):
    return service.create_dashboard_service(project_id, user_id, payload)


@router.get(
    "/{project_id}/dashboards/{dashboard_id}",
    response_model=DashboardOut,
    summary="Get a single dashboard",
)
def get_dashboard(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.get_dashboard_service(project_id, dashboard_id, user_id)


@router.put(
    "/{project_id}/dashboards/{dashboard_id}",
    response_model=DashboardOut,
    summary="Update an existing dashboard",
)
def update_dashboard(
    project_id: UUID,
    dashboard_id: UUID,
    payload: DashboardUpdate,
    user_id: str = Depends(get_current_user_id),
):
    return service.update_dashboard_service(project_id, dashboard_id, user_id, payload)--- ./dashboards/repository.py ---
# app/dashboards/repository.py
from typing import Any, Dict, List, Optional
from uuid import UUID

from psycopg2.extras import Json

from app.routers.projects import get_db_connection


def list_dashboards_for_project(
    project_id: UUID,
    user_id: str,
) -> List[Dict[str, Any]]:
    sql = """
        SELECT
            id, project_id, user_id,
            name, description, is_favorite,
            layout, overrides,
            created_at, updated_at
        FROM public.project_dashboards
        WHERE project_id = %s AND user_id = %s
        ORDER BY is_favorite DESC, created_at DESC;
    """
    with get_db_connection() as conn, conn.cursor() as cur:
        cur.execute(sql, (str(project_id), user_id))
        rows = cur.fetchall()
        if not rows:
            return []
        cols = [d[0] for d in cur.description]
        return [dict(zip(cols, r)) for r in rows]


def fetch_dashboard(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str,
) -> Optional[Dict[str, Any]]:
    sql = """
        SELECT
            id, project_id, user_id,
            name, description, is_favorite,
            layout, overrides,
            created_at, updated_at
        FROM public.project_dashboards
        WHERE id = %s AND project_id = %s AND user_id = %s
        LIMIT 1;
    """
    with get_db_connection() as conn, conn.cursor() as cur:
        cur.execute(sql, (str(dashboard_id), str(project_id), user_id))
        row = cur.fetchone()
        if not row:
            return None
        cols = [d[0] for d in cur.description]
        return dict(zip(cols, row))


def insert_dashboard(
    project_id: UUID,
    user_id: str,
    payload: Dict[str, Any],
) -> Dict[str, Any]:
    sql = """
        INSERT INTO public.project_dashboards (
            project_id, user_id,
            name, description, is_favorite,
            layout, overrides
        )
        VALUES (%s,%s,%s,%s,%s,%s,%s)
        RETURNING
            id, project_id, user_id,
            name, description, is_favorite,
            layout, overrides,
            created_at, updated_at;
    """
    with get_db_connection() as conn, conn.cursor() as cur:
        cur.execute(
            sql,
            (
                str(project_id),
                user_id,
                payload.get("name"),
                payload.get("description"),
                payload.get("is_favorite", False),
                Json(payload.get("layout")) if payload.get("layout") is not None else None,
                Json(payload.get("overrides")) if payload.get("overrides") is not None else None,
            ),
        )
        row = cur.fetchone()
        cols = [d[0] for d in cur.description]
        conn.commit()
        return dict(zip(cols, row))


def update_dashboard(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str,
    payload: Dict[str, Any],
) -> Optional[Dict[str, Any]]:
    sql = """
        UPDATE public.project_dashboards
        SET
            name = COALESCE(%s, name),
            description = COALESCE(%s, description),
            is_favorite = COALESCE(%s, is_favorite),
            layout = COALESCE(%s, layout),
            overrides = COALESCE(%s, overrides),
            updated_at = now()
        WHERE id = %s AND project_id = %s AND user_id = %s
        RETURNING
            id, project_id, user_id,
            name, description, is_favorite,
            layout, overrides,
            created_at, updated_at;
    """
    with get_db_connection() as conn, conn.cursor() as cur:
        cur.execute(
            sql,
            (
                payload.get("name"),
                payload.get("description"),
                payload.get("is_favorite"),
                Json(payload.get("layout")) if "layout" in payload else None,
                Json(payload.get("overrides")) if "overrides" in payload else None,
                str(dashboard_id),
                str(project_id),
                user_id,
            ),
        )
        row = cur.fetchone()
        if not row:
            return None
        cols = [d[0] for d in cur.description]
        conn.commit()
        return dict(zip(cols, row))--- ./scenarios/service.py ---
from uuid import UUID
from fastapi import HTTPException
from . import repository as repo
from .schemas import ForecastParametersOut, ForecastParametersPatch

def get_forecast(project_id: UUID, user_id: str) -> ForecastParametersOut:
    # 1. Ensure DB row exists (Lazy Creation)
    repo.ensure_assumptions_row(project_id, user_id)
    
    # 2. Fetch
    data = repo.get_forecast_params(project_id, user_id)
    if not data:
        raise HTTPException(404, "Forecast parameters not found")
        
    return ForecastParametersOut(**data)

def update_forecast(project_id: UUID, user_id: str, payload: ForecastParametersPatch) -> ForecastParametersOut:
    # 1. Ensure DB row exists
    repo.ensure_assumptions_row(project_id, user_id)
    
    # 2. Update
    data = payload.model_dump(exclude_unset=True)
    updated = repo.update_forecast_params(project_id, user_id, data)
    
    if not updated:
        raise HTTPException(500, "Failed to update parameters")
        
    return ForecastParametersOut(**updated)--- ./scenarios/__init__.py ---
# app/scenarios/__init__.py
from .router import router

__all__ = ["router"]--- ./scenarios/schemas.py ---
from pydantic import BaseModel
from typing import Optional
from uuid import UUID
from datetime import datetime

class ForecastParametersOut(BaseModel):
    id: UUID
    project_id: UUID
    
    # Section A: Economic Reality
    cpi_percentage: float = 6.0
    discount_rate: float = 8.0
    previous_allocation: float = 0
    
    # Section B: Engineering Reality
    paved_deterioration_rate: str = "Medium" # Slow, Medium, Fast
    gravel_loss_rate: float = 20.0          # mm/year
    climate_stress_factor: str = "Medium"   # Low, Medium, High
    
    # Section C: Time Machine
    analysis_duration: int = 5
    
    updated_at: datetime

class ForecastParametersPatch(BaseModel):
    cpi_percentage: Optional[float] = None
    discount_rate: Optional[float] = None
    previous_allocation: Optional[float] = None
    
    paved_deterioration_rate: Optional[str] = None
    gravel_loss_rate: Optional[float] = None
    climate_stress_factor: Optional[str] = None
    
    analysis_duration: Optional[int] = None--- ./scenarios/router.py ---
from fastapi import APIRouter, Depends
from uuid import UUID

from app.routers.projects import get_current_user_id
from .schemas import ForecastParametersOut, ForecastParametersPatch
from . import service

router = APIRouter()

@router.get(
    "/{project_id}/forecast",
    response_model=ForecastParametersOut,
    summary="Get forecast & strategy assumptions"
)
def get_forecast_parameters(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.get_forecast(project_id, user_id)

@router.patch(
    "/{project_id}/forecast",
    response_model=ForecastParametersOut,
    summary="Update forecast & strategy assumptions"
)
def update_forecast_parameters(
    project_id: UUID,
    payload: ForecastParametersPatch,
    user_id: str = Depends(get_current_user_id),
):
    return service.update_forecast(project_id, user_id, payload)--- ./scenarios/repository.py ---
from uuid import UUID
from typing import Dict, Any, Optional
from app.routers.projects import get_db_connection

def ensure_assumptions_row(project_id: UUID, user_id: str) -> None:
    """
    Ensures a row exists for this project in the assumptions table.
    """
    sql = """
        INSERT INTO public.scenario_assumptions (project_id, user_id)
        VALUES (%s, %s)
        ON CONFLICT (project_id) DO NOTHING;
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
        conn.commit()

def get_forecast_params(project_id: UUID, user_id: str) -> Optional[Dict[str, Any]]:
    sql = """
        SELECT 
            id, project_id, updated_at,
            analysis_duration, discount_rate,
            cpi_percentage, previous_allocation,
            paved_deterioration_rate, gravel_loss_rate, climate_stress_factor
        FROM public.scenario_assumptions
        WHERE project_id = %s AND user_id = %s
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            row = cur.fetchone()
            if not row:
                return None
            cols = [d[0] for d in cur.description]
            return dict(zip(cols, row))

def update_forecast_params(project_id: UUID, user_id: str, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    # Dynamic SQL construction
    set_clauses = []
    values = []
    for k, v in data.items():
        set_clauses.append(f"{k} = %s")
        values.append(v)
    
    if not set_clauses:
        return get_forecast_params(project_id, user_id)

    sql = f"""
        UPDATE public.scenario_assumptions
        SET {", ".join(set_clauses)}, updated_at = NOW()
        WHERE project_id = %s AND user_id = %s
        RETURNING 
            id, project_id, updated_at,
            analysis_duration, discount_rate,
            cpi_percentage, previous_allocation,
            paved_deterioration_rate, gravel_loss_rate, climate_stress_factor
    """
    
    values.extend([str(project_id), user_id])
    
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, tuple(values))
            row = cur.fetchone()
            conn.commit()
            if not row:
                return None
            cols = [d[0] for d in cur.description]
            return dict(zip(cols, row))--- ./db/models.py ---
# app/db/models.py

from pydantic import BaseModel
from typing import Optional
from datetime import datetime
from uuid import UUID
import os



JWT_SECRET = os.getenv("SUPABASE_JWT_SECRET")
ALGORITHM = "HS256"

print("JWT_SECRET loaded? ->", bool(JWT_SECRET))  


# Schema used for retrieving a saved project from the database
class ProjectDB(BaseModel):
    id: UUID
    user_id: UUID
    project_name: str
    description: Optional[str] = None
    start_year: int
    forecast_duration: int
    discount_rate: float # Numeric in PG, float in Python
    created_at: datetime
    updated_at: datetime
    
    class Config:
        orm_mode = True # Allows Pydantic to read ORM objects--- ./db/__init__.py ---
--- ./db/schemas.py ---
from pydantic import BaseModel
from typing import Optional
from datetime import datetime
from uuid import UUID


# ============================================================
# PROJECT Schemas (proposal-first flow)
# ============================================================

class ProjectMetadata(BaseModel):
    project_name: str
    province: str
    start_year: int


class ProjectDB(BaseModel):
    id: UUID
    user_id: UUID
    project_name: str
    province: str
    start_year: int

    proposal_title: Optional[str] = None
    proposal_status: Optional[str] = None

    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True--- ./ai_advisor/service.py ---
import os
import json
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# Initialize OpenAI Client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def generate_strategic_narrative(context_data: dict) -> dict:
    """
    Sends simulation data to OpenAI and returns a structured strategic review.
    """
    
    # 1. The Persona (Who the AI is)
    system_prompt = """
        You are a Senior Infrastructure Economist advising the Provincial Treasury.
            
            Your task is two-fold:
            1. Analyze specific graph data points (Condition Forecast, Financials).
            2. Write a formal strategic conclusion.

            Output Format: return ONLY a JSON object with this exact structure:
            {
                "chart_insights": {
                    "condition_forecast": "One short, sharp sentence analyzing the VCI trend (e.g. 'The rapid decline in 2028 indicates a structural failure point.').",
                    "budget_impact": "One short, sharp sentence on the financial reality (e.g. 'Current funding covers only 60% of depreciation.')."
                },
                "executive_summary": "A 2-sentence high-level summary of the crisis.",
                "risk_analysis": "A paragraph explaining the financial and engineering risks.",
                "economic_impact": "A short statement on broader economic effects.",
                "recommended_action": ["Action 1", "Action 2", "Action 3"]
            }
    """

    # 2. The Data (What the AI analyzes)
    user_prompt = f"""
    Analyze the following network simulation results for a funding proposal:
    
    - Project Name: {context_data.get('project_name', 'Provincial Road Network')}
    - Forecast Duration: {context_data.get('duration')} Years
    - Current Asset Value (CRC): {context_data.get('current_asset_value')}
    - Current Network Health (VCI): {context_data.get('start_vci')}
    
    SCENARIO OUTCOME (If this budget is approved):
    - Total Budget Required (NPV): {context_data.get('total_cost')}
    - Future Network Health (VCI): {context_data.get('end_vci')}
    - Health Change: {context_data.get('vci_change')} points
    - Inflation Rate Used: {context_data.get('inflation')}%
    
    If the VCI drops, emphasize the destruction of asset value. 
    If the VCI improves, emphasize the return on investment.
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o", # Or "gpt-3.5-turbo" if you want to save credits
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"}, # Forces clean JSON
            temperature=0.7
        )
        
        # Parse the JSON string back into a Python dictionary
        content = response.choices[0].message.content
        return json.loads(content)
        
    except Exception as e:
        print(f"OpenAI Error: {e}")
        # Fallback if API fails or runs out of credits
        return {
            "executive_summary": "Unable to generate AI analysis at this time.",
            "risk_analysis": "Please review the data manually.",
            "economic_impact": "N/A",
            "recommended_action": ["Review inputs", "Check API connection"]
        }--- ./ai_advisor/router.py ---
from fastapi import APIRouter, Depends, HTTPException
from uuid import UUID
import json
from app.routers.projects import get_current_user_id, get_db_connection
from .service import generate_strategic_narrative

router = APIRouter()

@router.get(
    "/{project_id}/advisor/generate",
    summary="Generate AI Strategic Feedback based on latest simulation"
)
def get_ai_feedback(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id)
):
    # 1. Fetch Project Meta (Name)
    project_name = "Unknown Project"
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute("SELECT project_name FROM projects WHERE id = %s", (str(project_id),))
            row = cur.fetchone()
            if row:
                project_name = row[0]

            # 2. Fetch Latest Simulation Result
            cur.execute("""
                SELECT results_payload 
                FROM simulation_results 
                WHERE project_id = %s 
                ORDER BY run_at DESC LIMIT 1
            """, (str(project_id),))
            row = cur.fetchone()
            
            if not row:
                raise HTTPException(404, "No simulation data found. Run a simulation first.")
                
            sim_data = row[0] # This is the JSON dict from the DB

    # 3. Simplify Data for the AI (Don't send huge arrays)
    # We extract only what matters for the story
    yearly = sim_data.get("yearly_data", [])
    if not yearly:
        raise HTTPException(400, "Corrupt simulation data.")

    start_vci = yearly[0].get("avg_condition_index")
    end_vci = yearly[-1].get("avg_condition_index")
    
    # Format huge numbers to readable strings (e.g., "R 40.5 Billion")
    def fmt_money(x):
        return f"R {x/1_000_000_000:.2f} Billion" if x > 1_000_000_000 else f"R {x/1_000_000:.1f} Million"

    context_payload = {
        "project_name": project_name,
        "duration": sim_data.get("year_count"),
        "current_asset_value": fmt_money(yearly[0].get("asset_value", 0)),
        "start_vci": start_vci,
        "end_vci": end_vci,
        "vci_change": round(end_vci - start_vci, 2),
        "total_cost": fmt_money(sim_data.get("total_cost_npv", 0)),
        "inflation": 6.0 # Hardcoded or fetch from forecast params if you prefer
    }

    # 4. Call the AI Brain
    ai_response = generate_strategic_narrative(context_payload)
    
    return ai_response--- ./main.py ---
# app/main.py

from pathlib import Path
from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

# -------------------------------------------------------------------
# 1. Load .env
# -------------------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent.parent
load_dotenv(BASE_DIR / ".env")

# -------------------------------------------------------------------
# 2. Router Imports
# -------------------------------------------------------------------
from app.routers.projects import router as projects_router
from app.proposal_data.router import router as proposal_data_router
from app.network_snapshot.router import router as network_snapshot_router
from app.scenarios.router import router as scenarios_router
from app.computation.router import router as computation_router
from app.ai_advisor import router as ai_router

# -------------------------------------------------------------------
# 3. App Config
# -------------------------------------------------------------------
app = FastAPI(
    title="Mosianedi Investment API",
    description="API Gateway for Provincial Road Budget Proposals & Forecasting.",
    version="1.0.0",
)

# -------------------------------------------------------------------
# 4. CORS Middleware (The Fix)
# -------------------------------------------------------------------
# You MUST list the specific domains. Wildcard "*" fails with credentials.
origins = [
    "http://localhost:3000",                      # Local Development
    "https://mosianedi-frontend.vercel.app",      # Production Frontend
    # Add any other preview URLs here if needed, e.g.:
    # "https://mosianedi-frontend-git-main.vercel.app" 
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,        # 👈 Specific origins, not ["*"]
    allow_credentials=True,       # Required for Supabase Auth headers
    allow_methods=["*"],
    allow_headers=["*"],
)

# -------------------------------------------------------------------
# 5. Register Routes
# -------------------------------------------------------------------
app.include_router(projects_router, prefix="/api/v1/projects", tags=["Projects"])
app.include_router(proposal_data_router, prefix="/api/v1/projects", tags=["Proposal Inputs"])
app.include_router(network_snapshot_router, prefix="/api/v1/projects", tags=["Network Snapshot"])
app.include_router(scenarios_router, prefix="/api/v1/projects", tags=["Forecast & Strategy"])
app.include_router(computation_router, prefix="/api/v1/projects", tags=["Computation Engine"])
app.include_router(ai_router.router, prefix="/api/v1/projects", tags=["AI Advisor"])

# -------------------------------------------------------------------
# 6. Health Check
# -------------------------------------------------------------------
@app.get("/api/health") # Renamed for clarity, often useful to have under /api
def health_check():
    return {"status": "online", "version": "1.0.0"}

@app.get("/")
def read_root():
    return {
        "status": "online",
        "service": "Mosianedi Investment API",
        "version": "1.0.0"
    }--- ./proposal_data/service.py ---
from .schemas import ProposalDataPayload

def compute_paved_total(p: ProposalDataPayload) -> float:
    return p.paved_arid + p.paved_semi_arid + p.paved_dry_sub_humid + p.paved_moist_sub_humid + p.paved_humid

def compute_gravel_total(p: ProposalDataPayload) -> float:
    return p.gravel_arid + p.gravel_semi_arid + p.gravel_dry_sub_humid + p.gravel_moist_sub_humid + p.gravel_humid
--- ./proposal_data/__init__.py ---
from .router import router--- ./proposal_data/schemas.py ---
from pydantic import BaseModel
from typing import Optional, Dict, Any
from datetime import datetime
from uuid import UUID


class ProposalDataOut(BaseModel):
    id: UUID
    project_id: UUID
    user_id: UUID
    data_source: str

    paved_arid: float = 0
    paved_semi_arid: float = 0
    paved_dry_sub_humid: float = 0
    paved_moist_sub_humid: float = 0
    paved_humid: float = 0

    gravel_arid: float = 0
    gravel_semi_arid: float = 0
    gravel_dry_sub_humid: float = 0
    gravel_moist_sub_humid: float = 0
    gravel_humid: float = 0

    avg_vci_used: float = 0
    vehicle_km: float = 0
    pct_vehicle_km_used: float = 0
    fuel_sales: float = 0
    pct_fuel_sales_used: float = 0
    fuel_option_selected: int = 1
    target_vci: float = 45

    extra_inputs: Dict[str, Any] = {}
    created_at: datetime
    updated_at: datetime


class ProposalDataPatch(BaseModel):
    # allow partial updates
    paved_arid: Optional[float] = None
    paved_semi_arid: Optional[float] = None
    paved_dry_sub_humid: Optional[float] = None
    paved_moist_sub_humid: Optional[float] = None
    paved_humid: Optional[float] = None

    gravel_arid: Optional[float] = None
    gravel_semi_arid: Optional[float] = None
    gravel_dry_sub_humid: Optional[float] = None
    gravel_moist_sub_humid: Optional[float] = None
    gravel_humid: Optional[float] = None

    avg_vci_used: Optional[float] = None
    vehicle_km: Optional[float] = None
    pct_vehicle_km_used: Optional[float] = None
    fuel_sales: Optional[float] = None
    pct_fuel_sales_used: Optional[float] = None
    fuel_option_selected: Optional[int] = None
    target_vci: Optional[float] = None

    extra_inputs: Optional[Dict[str, Any]] = None
    
    --- ./proposal_data/router.py ---
from uuid import UUID
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, Any

from app.routers.projects import get_current_user_id, get_db_connection
from .schemas import ProposalDataOut, ProposalDataPatch

router = APIRouter()

def _row_to_dict(cur, row) -> Dict[str, Any]:
    cols = [desc[0] for desc in cur.description]
    return dict(zip(cols, row))

def _ensure_row(project_id: UUID, user_id: str) -> None:
    sql = """
        INSERT INTO public.proposal_data (project_id, user_id, data_source)
        VALUES (%s, %s, 'manual')
        ON CONFLICT (project_id) DO NOTHING;
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
        conn.commit()

@router.get(
    "/{project_id}/proposal-data",
    response_model=ProposalDataOut,
    summary="Get proposal inputs for a project",
)
def get_proposal_data(project_id: UUID, user_id: str = Depends(get_current_user_id)):
    # ensure row exists (covers old projects)
    _ensure_row(project_id, user_id)

    sql = """
        SELECT *
        FROM public.proposal_data
        WHERE project_id = %s AND user_id = %s
        LIMIT 1;
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            row = cur.fetchone()
            if not row:
                raise HTTPException(404, "proposal_data row not found after ensure (unexpected).")
            return _row_to_dict(cur, row)

@router.patch(
    "/{project_id}/proposal-data",
    response_model=ProposalDataOut,
    summary="Update proposal inputs for a project",
)
def patch_proposal_data(
    project_id: UUID,
    payload: ProposalDataPatch,
    user_id: str = Depends(get_current_user_id),
):
    # ensure row exists (covers old projects)
    _ensure_row(project_id, user_id)

    data = payload.model_dump(exclude_unset=True)
    if not data:
        raise HTTPException(400, "No fields provided")

    set_parts = []
    values = []
    for k, v in data.items():
        set_parts.append(f"{k} = %s")
        values.append(v)

    set_clause = ", ".join(set_parts)

    sql = f"""
        UPDATE public.proposal_data
        SET {set_clause}, updated_at = now()
        WHERE project_id = %s AND user_id = %s
        RETURNING *;
    """

    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur:
                cur.execute(sql, (*values, str(project_id), user_id))
                row = cur.fetchone()
                if not row:
                    raise HTTPException(404, "proposal_data row not found for this project/user")
                conn.commit()
                return _row_to_dict(cur, row)
        except HTTPException:
            conn.rollback()
            raise
        except Exception as e:
            conn.rollback()
            raise HTTPException(500, f"Failed to update proposal_data: {str(e)}")
        --- ./proposal_data/repository.py ---
from typing import Optional, Dict, Any
from datetime import datetime

def assert_project_owned(conn, project_id: str, user_id: str) -> None:
    cur = conn.cursor()
    cur.execute(
        "SELECT 1 FROM public.projects WHERE id = %s AND user_id = %s",
        (project_id, user_id),
    )
    ok = cur.fetchone()
    cur.close()
    if not ok:
        raise ValueError("Project not found or not owned by user")

def get_proposal_data(conn, project_id: str, user_id: str) -> Optional[Dict[str, Any]]:
    # ownership check
    assert_project_owned(conn, project_id, user_id)

    cur = conn.cursor()
    cur.execute(
        """
        SELECT
          paved_arid, paved_semi_arid, paved_dry_sub_humid, paved_moist_sub_humid, paved_humid,
          gravel_arid, gravel_semi_arid, gravel_dry_sub_humid, gravel_moist_sub_humid, gravel_humid,
          avg_vci_used, vehicle_km, pct_vehicle_km_used,
          fuel_sales, pct_fuel_sales_used, fuel_option_selected, target_vci,
          extra_inputs, updated_at
        FROM public.proposal_data
        WHERE project_id = %s AND user_id = %s
        """,
        (project_id, user_id),
    )
    row = cur.fetchone()
    cols = [d[0] for d in cur.description]
    cur.close()

    if not row:
        return None

    data = dict(zip(cols, row))
    updated_at = data.get("updated_at")
    if isinstance(updated_at, datetime):
        data["updated_at"] = updated_at.isoformat()

    return data

def upsert_proposal_data(conn, project_id: str, user_id: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    assert_project_owned(conn, project_id, user_id)

    cur = conn.cursor()
    cur.execute(
        """
        INSERT INTO public.proposal_data (
          project_id, user_id, data_source,
          paved_arid, paved_semi_arid, paved_dry_sub_humid, paved_moist_sub_humid, paved_humid,
          gravel_arid, gravel_semi_arid, gravel_dry_sub_humid, gravel_moist_sub_humid, gravel_humid,
          avg_vci_used, vehicle_km, pct_vehicle_km_used,
          fuel_sales, pct_fuel_sales_used, fuel_option_selected, target_vci,
          extra_inputs, updated_at
        )
        VALUES (
          %s, %s, 'manual',
          %s, %s, %s, %s, %s,
          %s, %s, %s, %s, %s,
          %s, %s, %s,
          %s, %s, %s, %s,
          %s, now()
        )
        ON CONFLICT (project_id)
        DO UPDATE SET
          user_id = EXCLUDED.user_id,
          data_source = EXCLUDED.data_source,

          paved_arid = EXCLUDED.paved_arid,
          paved_semi_arid = EXCLUDED.paved_semi_arid,
          paved_dry_sub_humid = EXCLUDED.paved_dry_sub_humid,
          paved_moist_sub_humid = EXCLUDED.paved_moist_sub_humid,
          paved_humid = EXCLUDED.paved_humid,

          gravel_arid = EXCLUDED.gravel_arid,
          gravel_semi_arid = EXCLUDED.gravel_semi_arid,
          gravel_dry_sub_humid = EXCLUDED.gravel_dry_sub_humid,
          gravel_moist_sub_humid = EXCLUDED.gravel_moist_sub_humid,
          gravel_humid = EXCLUDED.gravel_humid,

          avg_vci_used = EXCLUDED.avg_vci_used,
          vehicle_km = EXCLUDED.vehicle_km,
          pct_vehicle_km_used = EXCLUDED.pct_vehicle_km_used,

          fuel_sales = EXCLUDED.fuel_sales,
          pct_fuel_sales_used = EXCLUDED.pct_fuel_sales_used,
          fuel_option_selected = EXCLUDED.fuel_option_selected,
          target_vci = EXCLUDED.target_vci,

          extra_inputs = EXCLUDED.extra_inputs,
          updated_at = now()
        RETURNING
          paved_arid, paved_semi_arid, paved_dry_sub_humid, paved_moist_sub_humid, paved_humid,
          gravel_arid, gravel_semi_arid, gravel_dry_sub_humid, gravel_moist_sub_humid, gravel_humid,
          avg_vci_used, vehicle_km, pct_vehicle_km_used,
          fuel_sales, pct_fuel_sales_used, fuel_option_selected, target_vci,
          extra_inputs, updated_at
        """,
        (
            project_id, user_id,
            payload["paved_arid"], payload["paved_semi_arid"], payload["paved_dry_sub_humid"], payload["paved_moist_sub_humid"], payload["paved_humid"],
            payload["gravel_arid"], payload["gravel_semi_arid"], payload["gravel_dry_sub_humid"], payload["gravel_moist_sub_humid"], payload["gravel_humid"],
            payload["avg_vci_used"], payload["vehicle_km"], payload["pct_vehicle_km_used"],
            payload["fuel_sales"], payload["pct_fuel_sales_used"], payload["fuel_option_selected"], payload["target_vci"],
            payload["extra_inputs"],
        ),
    )

    row = cur.fetchone()
    cols = [d[0] for d in cur.description]
    conn.commit()
    cur.close()

    data = dict(zip(cols, row))
    updated_at = data.get("updated_at")
    if isinstance(updated_at, datetime):
        data["updated_at"] = updated_at.isoformat()

    return data
--- ./proposal_data/validation.py ---
from typing import Dict, Any, List, Tuple
from .schemas import ProposalDataPayload
from .service import compute_paved_total, compute_gravel_total

class ProposalValidationError(Exception):
    def __init__(self, errors: List[Dict[str, Any]]):
        super().__init__("Proposal data validation failed")
        self.errors = errors

def validate_proposal_data(payload: ProposalDataPayload) -> Tuple[bool, List[Dict[str, Any]]]:
    errors: List[Dict[str, Any]] = []
    total = compute_paved_total(payload) + compute_gravel_total(payload)

    if total <= 0:
        errors.append({"field": "total_lane_km", "message": "Total network length must be > 0.", "code": "TOTAL_LANE_KM_ZERO"})

    if payload.avg_vci_used <= 0:
        errors.append({"field": "avg_vci_used", "message": "Average VCI must be > 0.", "code": "AVG_VCI_REQUIRED"})

    if payload.vehicle_km <= 0:
        errors.append({"field": "vehicle_km", "message": "Vehicle-km must be > 0.", "code": "VEHICLE_KM_REQUIRED"})

    if payload.fuel_sales <= 0:
        errors.append({"field": "fuel_sales", "message": "Fuel sales must be > 0.", "code": "FUEL_SALES_REQUIRED"})

    if payload.fuel_option_selected not in (1, 2):
        errors.append({"field": "fuel_option_selected", "message": "Fuel option must be 1 or 2.", "code": "INVALID_FUEL_OPTION"})

    return (len(errors) == 0), errors

def validate_or_raise(payload: ProposalDataPayload) -> None:
    ok, errors = validate_proposal_data(payload)
    if not ok:
        raise ProposalValidationError(errors)
