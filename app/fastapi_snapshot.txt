--- ./routers/__init__.py ---
--- ./routers/master_data.py ---
"""
Master Data Upload, Validation, History & Preview Endpoints
===========================================================

Clean, modular, production-ready version.
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from typing import Optional, Dict, Any, List
from uuid import UUID
from io import BytesIO

import pandas as pd
import psycopg2
from psycopg2.extras import Json

from pydantic import BaseModel

# Shared helpers
from app.routers.projects import get_db_connection, get_current_user_id
from app.db.schemas import MasterDataUploadStatus


# ============================================================
# PREVIEW SCHEMAS
# ============================================================

class DataPreviewResponse(BaseModel):
    preview_data: List[Dict[str, Any]]
    total_rows: int
    columns: List[str]


# ============================================================
# REQUIRED COLUMNS FOR MASTER DATA
# ============================================================

REQUIRED_COLUMNS = {
    "segment_id",
    "road_id",
    "road_class",
    "length_km",
    "surface_type",
}

router = APIRouter()


# ============================================================
# HELPERS
# ============================================================

def _parse_master_data_file(file_bytes: bytes, filename: str) -> pd.DataFrame:
    """
    Convert uploaded Excel/CSV file → pandas DataFrame.
    """
    buffer = BytesIO(file_bytes)
    name = filename.lower()

    try:
        if name.endswith((".xlsx", ".xls")):
            return pd.read_excel(buffer)
        elif name.endswith(".csv"):
            return pd.read_csv(buffer)
        else:
            raise HTTPException(
                status_code=400,
                detail="Invalid file type. Use .xlsx, .xls or .csv."
            )
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail=f"Could not parse file: {str(e)}"
        )


# ============================================================
# UPLOAD ENDPOINT
# ============================================================

@router.post("/{project_id}/master-data/upload", response_model=MasterDataUploadStatus)
async def upload_master_data(
    project_id: UUID,
    file: UploadFile = File(...),
    user_id: str = Depends(get_current_user_id),
):
    """
    Upload + validate a master data file.
    Saves the file blob inline in PostgreSQL.
    """
    file_bytes = await file.read()
    if not file_bytes:
        raise HTTPException(status_code=400, detail="Uploaded file is empty.")

    mime_type = file.content_type or "application/octet-stream"
    file_size = len(file_bytes)

    validation_errors: Dict[str, Any] = {}
    status = "validated"
    row_count: Optional[int] = None

    # ---- VALIDATION ----
    try:
        df = _parse_master_data_file(file_bytes, file.filename)
        row_count = len(df.index)

        df_columns_lower = {c.lower(): c for c in df.columns}
        missing = [col for col in REQUIRED_COLUMNS if col not in df_columns_lower]

        if missing:
            status = "failed"
            validation_errors["missing_columns"] = missing

    except HTTPException:
        status = "failed"
        raise
    except Exception as e:
        status = "failed"
        validation_errors["validation_error"] = f"Unexpected: {str(e)}"

    # ---- INSERT INTO DB ----
    sql = """
        INSERT INTO public.master_data_uploads (
            project_id, user_id,
            original_filename, mime_type, file_size,
            storage_strategy, file_blob,
            status, row_count, validation_errors
        )
        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
        RETURNING
            id, project_id, user_id, original_filename, mime_type,
            file_size, status, row_count, validation_errors, created_at;
    """

    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (
                    str(project_id),
                    user_id,
                    file.filename,
                    mime_type,
                    file_size,
                    "inline",
                    psycopg2.Binary(file_bytes),
                    status,
                    row_count,
                    Json(validation_errors) if validation_errors else None
                ))

                record = dict(zip([d[0] for d in cur.description], cur.fetchone()))
                conn.commit()
                return record

    except Exception as e:
        print("[MASTER_DATA] Upload DB error:", e)
        raise HTTPException(500, "Internal server error during upload.")


# ============================================================
# LAST UPLOAD ENDPOINT
# ============================================================

@router.get("/{project_id}/master-data/last-upload", response_model=MasterDataUploadStatus)
async def get_last_master_data_upload(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id)
):
    """
    Return metadata for the most recent upload.
    """
    sql = """
        SELECT
            id, project_id, user_id,
            original_filename, mime_type, file_size,
            status, row_count, validation_errors, created_at
        FROM public.master_data_uploads
        WHERE project_id = %s AND user_id = %s
        ORDER BY created_at DESC
        LIMIT 1;
    """

    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (str(project_id), user_id))
                row = cur.fetchone()

                if not row:
                    raise HTTPException(404, "No master data uploads found.")

                record = dict(zip([d[0] for d in cur.description], row))
                return record

    except Exception as e:
        print("[MASTER_DATA] Last upload fetch error:", e)
        raise HTTPException(500, "Internal server error fetching last upload.")


# ============================================================
# PREVIEW ENDPOINT (LATEST FILE)
# ============================================================

@router.get("/{project_id}/master-data/preview", response_model=DataPreviewResponse)
async def get_master_data_preview(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id)
):
    """
    Reads the file_blobs of the most recent upload and returns 
    up to 50 preview rows.
    """
    sql = """
        SELECT file_blob, original_filename
        FROM public.master_data_uploads
        WHERE project_id = %s AND user_id = %s
        ORDER BY created_at DESC
        LIMIT 1;
    """

    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(sql, (str(project_id), user_id))
                row = cur.fetchone()

                if not row:
                    raise HTTPException(404, "No file found for preview.")

                file_blob_raw, filename = row

                if isinstance(file_blob_raw, memoryview):
                    file_bytes = file_blob_raw.tobytes()
                else:
                    file_bytes = file_blob_raw

    except Exception as e:
        print("[PREVIEW] DB error:", e)
        raise HTTPException(500, "Database error fetching file for preview.")

    # ---- PARSE ----
    try:
        df = _parse_master_data_file(file_bytes, filename)
        total_rows = len(df.index)

        preview_df = df.head(50).fillna("")
        preview_data = preview_df.to_dict("records")

        return DataPreviewResponse(
            preview_data=preview_data,
            total_rows=total_rows,
            columns=list(df.columns)
        )
    except Exception as e:
        print("[PREVIEW] Pandas error:", e)
        raise HTTPException(500, f"Error processing preview: {str(e)}")--- ./routers/projects.py ---
# app/routers/projects.py

from fastapi import APIRouter, HTTPException, Depends, Header
from decimal import Decimal
import os
import psycopg2
from contextlib import contextmanager
from jose import jwt, JWTError

from app.db.schemas import ProjectMetadata, ProjectDB 

router = APIRouter()

JWT_SECRET = os.getenv("SUPABASE_JWT_SECRET")
ALGORITHM = "HS256"

@contextmanager
def get_db_connection():
    DB_URL = os.getenv("DATABASE_URL")
    if not DB_URL:
        raise ValueError("DATABASE_URL missing")

    conn = None
    try:
        conn = psycopg2.connect(DB_URL)
        yield conn
    finally:
        if conn:
            conn.close()

def get_current_user_id(authorization: str = Header(None)) -> str:
    if not authorization:
        raise HTTPException(401, "Authorization header missing")
    try:
        scheme, token = authorization.split()
        payload = jwt.decode(token, JWT_SECRET, algorithms=[ALGORITHM], options={"verify_aud": False})
        return payload.get("sub")
    except:
        raise HTTPException(401, "Invalid token")

@router.post("/", status_code=201)
async def create_project(metadata: ProjectMetadata, user_id: str = Depends(get_current_user_id)):
    sql = """
        INSERT INTO public.projects 
        (user_id, project_name, description, start_year, forecast_duration, discount_rate)
        VALUES (%s, %s, %s, %s, %s, %s)
        RETURNING id, created_at;
    """

    data = (
        user_id,
        metadata.project_name,
        metadata.description,
        metadata.start_year,
        metadata.forecast_duration,
        Decimal(str(metadata.discount_rate)),
    )

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, data)
            project_id, created_at = cur.fetchone()
            conn.commit()

    return {
        "project_id": str(project_id),
        "message": "Project created",
        "created_at": created_at.isoformat(),
    }

@router.get("/", response_model=list[ProjectDB])
async def list_projects(user_id: str = Depends(get_current_user_id)):
    sql = """
        SELECT id, user_id, project_name, description, start_year, forecast_duration, 
               discount_rate, created_at, updated_at
        FROM public.projects
        WHERE user_id = %s
        ORDER BY created_at DESC;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (user_id,))
            columns = [c[0] for c in cur.description]
            rows = cur.fetchall()

    projects = []
    for row in rows:
        d = dict(zip(columns, row))
        if isinstance(d["discount_rate"], Decimal):
            d["discount_rate"] = float(d["discount_rate"])
        projects.append(d)

    return projects--- ./__init__.py ---
--- ./network_snapshot/service.py ---
# app/network_snapshot/service.py

from uuid import UUID
from typing import Optional, Tuple, Any, List, Dict

import json
import pandas as pd
from fastapi import HTTPException

from app.routers.projects import get_db_connection
from app.master_data.validation import parse_master_data_file  # fallback for legacy uploads
from .schemas import (
    NetworkSnapshot,
    LengthByCategory,
    AssetValueByCategory,
    UnitCostByCategory,
)


# -------------------------------------------------
# DB helper: latest upload record + workbook blob
# -------------------------------------------------


def _fetch_latest_master_data_record(
    project_id: UUID,
    user_id: str,
) -> Tuple[UUID, Optional[Dict[str, Any]], Optional[bytes], Optional[str]]:
    """
    Fetch the latest master_data_uploads row for this project+user.

    Returns:
        upload_id,
        workbook_payload (dict or None),
        file_bytes (from file_blob, or None),
        original_filename (or None)
    """
    sql = """
        SELECT
            id,
            workbook_payload,
            file_blob,
            original_filename
        FROM public.master_data_uploads
        WHERE project_id = %s AND user_id = %s
        ORDER BY created_at DESC
        LIMIT 1;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            row = cur.fetchone()

            if not row:
                raise HTTPException(
                    status_code=404,
                    detail="No master data uploads found for this project.",
                )

            upload_id, workbook_payload_raw, file_blob_raw, original_filename = row

            # workbook_payload is usually a dict (psycopg2 Json), but be defensive
            workbook_payload: Optional[Dict[str, Any]] = None
            if workbook_payload_raw is not None:
                if isinstance(workbook_payload_raw, dict):
                    workbook_payload = workbook_payload_raw
                else:
                    try:
                        workbook_payload = json.loads(workbook_payload_raw)
                    except Exception:
                        workbook_payload = None

            # Convert file_blob to bytes if present
            file_bytes: Optional[bytes] = None
            if file_blob_raw is not None:
                if isinstance(file_blob_raw, memoryview):
                    file_bytes = file_blob_raw.tobytes()
                else:
                    file_bytes = file_blob_raw

            return upload_id, workbook_payload, file_bytes, original_filename


# -------------------------------------------------
# Generic helpers
# -------------------------------------------------


def _compute_length_breakdown(
    df: pd.DataFrame,
    column: str,
) -> List[LengthByCategory]:
    """
    Group by a categorical column (e.g. road_class, surface_type)
    and sum length_km.
    """
    if column not in df.columns or "length_km" not in df.columns:
        return []

    grouped = (
        df.groupby(column)["length_km"]
        .sum()
        .reset_index()
        .sort_values("length_km", ascending=False)
    )

    results: List[LengthByCategory] = []
    for _, row in grouped.iterrows():
        label = str(row[column])
        length = float(row["length_km"] or 0)
        results.append(LengthByCategory(label=label, length_km=length))

    return results


def _get_segments_df_from_workbook(
    workbook_payload: Dict[str, Any]
) -> Optional[pd.DataFrame]:
    """
    Build a DataFrame from workbook_payload['segments'] if present.
    """
    segments = workbook_payload.get("segments")
    if not segments:
        return None

    df = pd.DataFrame(segments)
    df.columns = [str(c).strip().lower() for c in df.columns]

    if "length_km" in df.columns:
        df["length_km"] = pd.to_numeric(df["length_km"], errors="coerce").fillna(0)

    return df


def _get_sheet_df(
    workbook_payload: Dict[str, Any],
    sheet_name: str,
) -> Optional[pd.DataFrame]:
    """
    Small helper: safely convert a workbook sheet to DataFrame.
    """
    if not workbook_payload or sheet_name not in workbook_payload:
        return None

    data = workbook_payload.get(sheet_name)
    if not data:
        return None

    df = pd.DataFrame(data)
    df.columns = [str(c).strip().lower() for c in df.columns]
    return df


def _find_first_numeric_column(
    df: pd.DataFrame,
    preferred_names: List[str],
) -> Optional[str]:
    """
    Tries a list of preferred column names; if none exist, picks
    the first numeric column, else None.
    """
    cols = set(df.columns)

    for name in preferred_names:
        if name in cols:
            return name

    # Fallback: any numeric-looking column
    numeric_cols = df.select_dtypes(include=["number"]).columns
    if len(numeric_cols) > 0:
        return numeric_cols[0]

    # Try to coerce object columns
    for col in df.columns:
        try:
            pd.to_numeric(df[col], errors="raise")
            return col
        except Exception:
            continue

    return None


# -------------------------------------------------
# Extra metrics by sheet
# -------------------------------------------------


def _compute_network_length_metrics(
    workbook_payload: Optional[Dict[str, Any]],
) -> tuple[Optional[float], List[LengthByCategory]]:
    """
    Use 'network_length' sheet if present.
    """
    if not workbook_payload:
        return None, []

    df = _get_sheet_df(workbook_payload, "network_length")
    if df is None or df.empty:
        return None, []

    # Ensure numeric length column
    length_col = _find_first_numeric_column(
        df,
        preferred_names=["length_km", "network_length_km", "length"],
    )
    if not length_col:
        return None, []

    df[length_col] = pd.to_numeric(df[length_col], errors="coerce").fillna(0)

    total_network_length_km = float(df[length_col].sum())

    # Category for breakdown – try a few expected options
    category_col = None
    for candidate in ["network_type", "road_class", "surface_type"]:
        if candidate in df.columns:
            category_col = candidate
            break

    length_by_network_type: List[LengthByCategory] = []
    if category_col:
        grouped = (
            df.groupby(category_col)[length_col]
            .sum()
            .reset_index()
            .sort_values(length_col, ascending=False)
        )
        for _, row in grouped.iterrows():
            label = str(row[category_col])
            length = float(row[length_col] or 0)
            length_by_network_type.append(
                LengthByCategory(label=label, length_km=length)
            )

    return total_network_length_km, length_by_network_type


def _compute_asset_value_metrics(
    workbook_payload: Optional[Dict[str, Any]],
) -> tuple[Optional[float], List[AssetValueByCategory]]:
    """
    Use 'asset_value' sheet if present.
    """
    if not workbook_payload:
        return None, []

    df = _get_sheet_df(workbook_payload, "asset_value")
    if df is None or df.empty:
        return None, []

    value_col = _find_first_numeric_column(
        df,
        preferred_names=["asset_value", "value", "total_value"],
    )
    if not value_col:
        return None, []

    df[value_col] = pd.to_numeric(df[value_col], errors="coerce").fillna(0)

    total_asset_value = float(df[value_col].sum())

    # Category – pick something readable if present
    category_col = None
    for candidate in ["asset_type", "network_type", "road_class", "surface_type"]:
        if candidate in df.columns:
            category_col = candidate
            break

    asset_value_by_category: List[AssetValueByCategory] = []
    if category_col:
        grouped = (
            df.groupby(category_col)[value_col]
            .sum()
            .reset_index()
            .sort_values(value_col, ascending=False)
        )
        for _, row in grouped.iterrows():
            label = str(row[category_col])
            value = float(row[value_col] or 0)
            asset_value_by_category.append(
                AssetValueByCategory(label=label, value=value)
            )

    return total_asset_value, asset_value_by_category


def _compute_unit_cost_metrics(
    workbook_payload: Optional[Dict[str, Any]],
) -> List[UnitCostByCategory]:
    """
    Use 'road_costs' sheet if present.

    We look for a cost-per-km style column and group by surface_type or
    maintenance_type, depending on what exists.
    """
    if not workbook_payload:
        return []

    df = _get_sheet_df(workbook_payload, "road_costs")
    if df is None or df.empty:
        return []

    cost_col = _find_first_numeric_column(
        df,
        preferred_names=[
            "cost_per_km",
            "unit_cost_per_km",
            "unit_cost",
            "cost",
        ],
    )
    if not cost_col:
        return []

    df[cost_col] = pd.to_numeric(df[cost_col], errors="coerce").fillna(0)

    # Choose a category column
    category_col = None
    for candidate in ["surface_type", "treatment_type", "maintenance_type", "road_class"]:
        if candidate in df.columns:
            category_col = candidate
            break

    if not category_col:
        # No category → summarise a single “all” bucket
        avg_cost = float(df[cost_col].mean())
        return [
            UnitCostByCategory(label="All", cost_per_km=avg_cost),
        ]

    grouped = (
        df.groupby(category_col)[cost_col]
        .mean()
        .reset_index()
        .sort_values(cost_col, ascending=False)
    )

    results: List[UnitCostByCategory] = []
    for _, row in grouped.iterrows():
        label = str(row[category_col])
        cost = float(row[cost_col] or 0)
        results.append(UnitCostByCategory(label=label, cost_per_km=cost))

    return results


# -------------------------------------------------
# Public service
# -------------------------------------------------


def get_network_snapshot(
    project_id: UUID,
    user_id: str,
) -> NetworkSnapshot:
    """
    Compute a lightweight 'network snapshot' for the latest
    master data upload.

    Sources:
    - segments sheet      → core totals + breakdowns
    - network_length      → total_network_length_km, length_by_network_type
    - asset_value         → total_asset_value, asset_value_by_category
    - road_costs          → unit_costs_by_surface
    """
    # 1) Get latest upload record
    upload_id, workbook_payload, file_bytes, filename = _fetch_latest_master_data_record(
        project_id=project_id,
        user_id=user_id,
    )

    # 2) Get segments DataFrame
    df_segments: Optional[pd.DataFrame] = None
    if workbook_payload:
        df_segments = _get_segments_df_from_workbook(workbook_payload)

    # Fallback to blob (legacy uploads)
    if df_segments is None:
        if not file_bytes or not filename:
            raise HTTPException(
                status_code=500,
                detail="No usable workbook payload or file blob found for snapshot.",
            )
        df_segments = parse_master_data_file(file_bytes, filename)
        df_segments.columns = [str(c).strip().lower() for c in df_segments.columns]
        if "length_km" in df_segments.columns:
            df_segments["length_km"] = pd.to_numeric(
                df_segments["length_km"], errors="coerce"
            ).fillna(0)

    if "length_km" not in df_segments.columns:
        raise HTTPException(
            status_code=400,
            detail="The master data (segments) does not contain a 'length_km' column.",
        )

    # 3) Core metrics from segments
    total_segments = int(len(df_segments.index))
    total_length_km = float(df_segments["length_km"].sum())

    total_roads: Optional[int] = None
    if "road_id" in df_segments.columns:
        total_roads = int(df_segments["road_id"].nunique())

    length_by_road_class = (
        _compute_length_breakdown(df_segments, "road_class")
        if "road_class" in df_segments.columns
        else []
    )
    length_by_surface_type = (
        _compute_length_breakdown(df_segments, "surface_type")
        if "surface_type" in df_segments.columns
        else []
    )

    # 4) Extra metrics from other sheets
    total_network_length_km, length_by_network_type = _compute_network_length_metrics(
        workbook_payload
    )
    total_asset_value, asset_value_by_category = _compute_asset_value_metrics(
        workbook_payload
    )
    unit_costs_by_surface = _compute_unit_cost_metrics(workbook_payload)

    # 5) Build response
    return NetworkSnapshot(
        project_id=project_id,
        upload_id=upload_id,
        total_length_km=total_length_km,
        total_segments=total_segments,
        total_roads=total_roads,
        length_by_road_class=length_by_road_class,
        length_by_surface_type=length_by_surface_type,
        total_network_length_km=total_network_length_km,
        length_by_network_type=length_by_network_type,
        total_asset_value=total_asset_value,
        asset_value_by_category=asset_value_by_category,
        unit_costs_by_surface=unit_costs_by_surface,
    )--- ./network_snapshot/__init__.py ---
# app/network_snapshot/__init__.py
from .router import router  # re-export for easy import in main.py

__all__ = ["router"]--- ./network_snapshot/schemas.py ---
# app/network_snapshot/schemas.py

from uuid import UUID
from typing import List, Optional

from pydantic import BaseModel, Field


class LengthByCategory(BaseModel):
    label: str
    length_km: float


class AssetValueByCategory(BaseModel):
    label: str
    value: float


class UnitCostByCategory(BaseModel):
    label: str
    cost_per_km: float


class NetworkSnapshot(BaseModel):
    project_id: UUID
    upload_id: UUID

    # --- Core from segments sheet ---
    total_length_km: float
    total_segments: int
    total_roads: Optional[int] = None

    length_by_road_class: List[LengthByCategory] = Field(default_factory=list)
    length_by_surface_type: List[LengthByCategory] = Field(default_factory=list)

    # --- From network_length sheet (optional) ---
    total_network_length_km: Optional[float] = None
    length_by_network_type: List[LengthByCategory] = Field(default_factory=list)

    # --- From asset_value sheet (optional) ---
    total_asset_value: Optional[float] = None
    asset_value_by_category: List[AssetValueByCategory] = Field(default_factory=list)

    # --- From road_costs sheet (optional) ---
    unit_costs_by_surface: List[UnitCostByCategory] = Field(default_factory=list)--- ./network_snapshot/router.py ---
# app/network_snapshot/router.py

from uuid import UUID
from fastapi import APIRouter, Depends

from app.routers.projects import get_current_user_id
from .schemas import NetworkSnapshot
from .service import get_network_snapshot

router = APIRouter()


@router.get(
    "/{project_id}/network/snapshot",
    response_model=NetworkSnapshot,
    summary="Get network snapshot for latest master data upload",
)
def read_network_snapshot(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    """
    Returns high-level network metrics (total length, segments,
    breakdown by class and surface) computed from the latest
    master data upload for this project & user.
    """
    return get_network_snapshot(project_id=project_id, user_id=user_id)--- ./dashboards/service.py ---
# app/dashboards/service.py
from typing import List
from uuid import UUID

from fastapi import HTTPException

from .schemas import DashboardCreate, DashboardOut, DashboardUpdate
from . import repository


def list_dashboards_service(
    project_id: UUID,
    user_id: str,
) -> List[DashboardOut]:
    rows = repository.list_dashboards_for_project(project_id, user_id)
    return [DashboardOut(**r) for r in rows]


def get_dashboard_service(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str,
) -> DashboardOut:
    row = repository.fetch_dashboard(project_id, dashboard_id, user_id)
    if not row:
        raise HTTPException(status_code=404, detail="Dashboard not found.")
    return DashboardOut(**row)


def create_dashboard_service(
    project_id: UUID,
    user_id: str,
    data: DashboardCreate,
) -> DashboardOut:
    payload = data.model_dump()
    row = repository.insert_dashboard(project_id, user_id, payload)
    return DashboardOut(**row)


def update_dashboard_service(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str,
    data: DashboardUpdate,
) -> DashboardOut:
    payload = {k: v for k, v in data.model_dump().items() if v is not None}
    row = repository.update_dashboard(project_id, dashboard_id, user_id, payload)
    if not row:
        raise HTTPException(status_code=404, detail="Dashboard not found.")
    return DashboardOut(**row)--- ./dashboards/__init__.py ---
# app/dashboards/__init__.py
from .router import router  # so main.py can `from app.dashboards import router`

__all__ = ["router"]--- ./dashboards/schemas.py ---
# app/dashboards/schemas.py
from typing import Any, Dict, Optional
from uuid import UUID

from pydantic import BaseModel, Field


class DashboardBase(BaseModel):
    name: str = Field(..., max_length=200)
    description: Optional[str] = None
    is_favorite: bool = False
    layout: Optional[Dict[str, Any]] = None
    overrides: Optional[Dict[str, Any]] = None


class DashboardCreate(DashboardBase):
    """Payload for creating a dashboard."""
    pass


class DashboardUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    is_favorite: Optional[bool] = None
    layout: Optional[Dict[str, Any]] = None
    overrides: Optional[Dict[str, Any]] = None


class DashboardOut(DashboardBase):
    id: UUID
    project_id: UUID
    user_id: str
    created_at: str
    updated_at: str

    class Config:
        from_attributes = True--- ./dashboards/router.py ---
# app/dashboards/router.py
from typing import List
from uuid import UUID

from fastapi import APIRouter, Depends

from app.routers.projects import get_current_user_id
from .schemas import DashboardCreate, DashboardOut, DashboardUpdate
from . import service

router = APIRouter()


@router.get(
    "/{project_id}/dashboards",
    response_model=List[DashboardOut],
    summary="List dashboards for a project",
)
def list_dashboards(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.list_dashboards_service(project_id, user_id)


@router.post(
    "/{project_id}/dashboards",
    response_model=DashboardOut,
    summary="Create a new dashboard",
)
def create_dashboard(
    project_id: UUID,
    payload: DashboardCreate,
    user_id: str = Depends(get_current_user_id),
):
    return service.create_dashboard_service(project_id, user_id, payload)


@router.get(
    "/{project_id}/dashboards/{dashboard_id}",
    response_model=DashboardOut,
    summary="Get a single dashboard",
)
def get_dashboard(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.get_dashboard_service(project_id, dashboard_id, user_id)


@router.put(
    "/{project_id}/dashboards/{dashboard_id}",
    response_model=DashboardOut,
    summary="Update an existing dashboard",
)
def update_dashboard(
    project_id: UUID,
    dashboard_id: UUID,
    payload: DashboardUpdate,
    user_id: str = Depends(get_current_user_id),
):
    return service.update_dashboard_service(project_id, dashboard_id, user_id, payload)--- ./dashboards/repository.py ---
# app/dashboards/repository.py
from typing import Any, Dict, List, Optional
from uuid import UUID

from psycopg2.extras import Json

from app.routers.projects import get_db_connection


def list_dashboards_for_project(
    project_id: UUID,
    user_id: str,
) -> List[Dict[str, Any]]:
    sql = """
        SELECT
            id, project_id, user_id,
            name, description, is_favorite,
            layout, overrides,
            created_at, updated_at
        FROM public.project_dashboards
        WHERE project_id = %s AND user_id = %s
        ORDER BY is_favorite DESC, created_at DESC;
    """
    with get_db_connection() as conn, conn.cursor() as cur:
        cur.execute(sql, (str(project_id), user_id))
        rows = cur.fetchall()
        if not rows:
            return []
        cols = [d[0] for d in cur.description]
        return [dict(zip(cols, r)) for r in rows]


def fetch_dashboard(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str,
) -> Optional[Dict[str, Any]]:
    sql = """
        SELECT
            id, project_id, user_id,
            name, description, is_favorite,
            layout, overrides,
            created_at, updated_at
        FROM public.project_dashboards
        WHERE id = %s AND project_id = %s AND user_id = %s
        LIMIT 1;
    """
    with get_db_connection() as conn, conn.cursor() as cur:
        cur.execute(sql, (str(dashboard_id), str(project_id), user_id))
        row = cur.fetchone()
        if not row:
            return None
        cols = [d[0] for d in cur.description]
        return dict(zip(cols, row))


def insert_dashboard(
    project_id: UUID,
    user_id: str,
    payload: Dict[str, Any],
) -> Dict[str, Any]:
    sql = """
        INSERT INTO public.project_dashboards (
            project_id, user_id,
            name, description, is_favorite,
            layout, overrides
        )
        VALUES (%s,%s,%s,%s,%s,%s,%s)
        RETURNING
            id, project_id, user_id,
            name, description, is_favorite,
            layout, overrides,
            created_at, updated_at;
    """
    with get_db_connection() as conn, conn.cursor() as cur:
        cur.execute(
            sql,
            (
                str(project_id),
                user_id,
                payload.get("name"),
                payload.get("description"),
                payload.get("is_favorite", False),
                Json(payload.get("layout")) if payload.get("layout") is not None else None,
                Json(payload.get("overrides")) if payload.get("overrides") is not None else None,
            ),
        )
        row = cur.fetchone()
        cols = [d[0] for d in cur.description]
        conn.commit()
        return dict(zip(cols, row))


def update_dashboard(
    project_id: UUID,
    dashboard_id: UUID,
    user_id: str,
    payload: Dict[str, Any],
) -> Optional[Dict[str, Any]]:
    sql = """
        UPDATE public.project_dashboards
        SET
            name = COALESCE(%s, name),
            description = COALESCE(%s, description),
            is_favorite = COALESCE(%s, is_favorite),
            layout = COALESCE(%s, layout),
            overrides = COALESCE(%s, overrides),
            updated_at = now()
        WHERE id = %s AND project_id = %s AND user_id = %s
        RETURNING
            id, project_id, user_id,
            name, description, is_favorite,
            layout, overrides,
            created_at, updated_at;
    """
    with get_db_connection() as conn, conn.cursor() as cur:
        cur.execute(
            sql,
            (
                payload.get("name"),
                payload.get("description"),
                payload.get("is_favorite"),
                Json(payload.get("layout")) if "layout" in payload else None,
                Json(payload.get("overrides")) if "overrides" in payload else None,
                str(dashboard_id),
                str(project_id),
                user_id,
            ),
        )
        row = cur.fetchone()
        if not row:
            return None
        cols = [d[0] for d in cur.description]
        conn.commit()
        return dict(zip(cols, row))--- ./scenarios/service.py ---
# app/scenarios/service.py

from uuid import UUID
from typing import List

from fastapi import HTTPException

from .schemas import (
    ScenarioCreate,
    ScenarioUpdate,
    ScenarioRead,
    ScenarioSummary,
)
from . import repository as repo


def create_scenario(
    project_id: UUID,
    user_id: str,
    payload: ScenarioCreate,
) -> ScenarioRead:
    data = repo.create_scenario(project_id, user_id, payload.dict())
    return ScenarioRead(**data)


def list_scenarios(
    project_id: UUID,
    user_id: str,
) -> List[ScenarioSummary]:
    rows = repo.list_scenarios(project_id, user_id)
    return [
        ScenarioSummary(
            id=row["id"],
            name=row["name"],
            is_baseline=row["is_baseline"],
            created_at=row["created_at"],
        )
        for row in rows
    ]


def get_scenario(
    project_id: UUID,
    scenario_id: UUID,
    user_id: str,
) -> ScenarioRead:
    row = repo.get_scenario(project_id, scenario_id, user_id)
    if not row:
        raise HTTPException(status_code=404, detail="Scenario not found.")
    return ScenarioRead(**row)


def update_scenario(
    project_id: UUID,
    scenario_id: UUID,
    user_id: str,
    payload: ScenarioUpdate,
) -> ScenarioRead:
    row = repo.update_scenario(project_id, scenario_id, user_id, payload.dict(exclude_unset=True))
    if not row:
        raise HTTPException(status_code=404, detail="Scenario not found.")
    return ScenarioRead(**row)


def delete_scenario(
    project_id: UUID,
    scenario_id: UUID,
    user_id: str,
) -> None:
    ok = repo.delete_scenario(project_id, scenario_id, user_id)
    if not ok:
        raise HTTPException(status_code=404, detail="Scenario not found.")--- ./scenarios/__init__.py ---
# app/scenarios/__init__.py
from .router import router

__all__ = ["router"]--- ./scenarios/schemas.py ---
# app/scenarios/schemas.py

from uuid import UUID
from typing import Any, Dict, Optional, List
from datetime import datetime

from pydantic import BaseModel, Field


class ScenarioAssumptionsBase(BaseModel):
    """
    'parameters' is where we keep actual sliders/inputs, e.g.
    {
      "analysis_period_years": 20,
      "discount_rate": 0.08,
      "annual_budget_million": 250,
      "unit_cost_reseal_per_km": 0.8,
      "unit_cost_reconstruct_per_km": 3.5,
      "routine_maintenance_pct": 0.03
    }
    """
    name: str
    description: Optional[str] = None
    is_baseline: bool = False
    parameters: Dict[str, Any] = Field(default_factory=dict)


class ScenarioCreate(ScenarioAssumptionsBase):
    pass


class ScenarioUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    is_baseline: Optional[bool] = None
    parameters: Optional[Dict[str, Any]] = None


class ScenarioRead(ScenarioAssumptionsBase):
    id: UUID
    project_id: UUID
    user_id: str
    created_at: datetime
    updated_at: datetime


class ScenarioSummary(BaseModel):
    id: UUID
    name: str
    is_baseline: bool
    created_at: datetime--- ./scenarios/router.py ---
# app/scenarios/router.py

from uuid import UUID
from typing import List

from fastapi import APIRouter, Depends, status

from app.routers.projects import get_current_user_id
from .schemas import (
    ScenarioCreate,
    ScenarioUpdate,
    ScenarioRead,
    ScenarioSummary,
)
from . import service

router = APIRouter()


@router.post(
    "/{project_id}/scenarios",
    response_model=ScenarioRead,
    status_code=status.HTTP_201_CREATED,
    summary="Create a new scenario for this project",
)
def create_project_scenario(
    project_id: UUID,
    payload: ScenarioCreate,
    user_id: str = Depends(get_current_user_id),
):
    return service.create_scenario(project_id, user_id, payload)


@router.get(
    "/{project_id}/scenarios",
    response_model=List[ScenarioSummary],
    summary="List all scenarios for this project",
)
def list_project_scenarios(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.list_scenarios(project_id, user_id)


@router.get(
    "/{project_id}/scenarios/{scenario_id}",
    response_model=ScenarioRead,
    summary="Get a single scenario with full assumptions",
)
def get_project_scenario(
    project_id: UUID,
    scenario_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.get_scenario(project_id, scenario_id, user_id)


@router.put(
    "/{project_id}/scenarios/{scenario_id}",
    response_model=ScenarioRead,
    summary="Update a scenario",
)
def update_project_scenario(
    project_id: UUID,
    scenario_id: UUID,
    payload: ScenarioUpdate,
    user_id: str = Depends(get_current_user_id),
):
    return service.update_scenario(project_id, scenario_id, user_id, payload)


@router.delete(
    "/{project_id}/scenarios/{scenario_id}",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="Delete a scenario",
)
def delete_project_scenario(
    project_id: UUID,
    scenario_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    service.delete_scenario(project_id, scenario_id, user_id)
    return--- ./scenarios/repository.py ---
# app/scenarios/repository.py

from uuid import UUID
from typing import List, Dict, Any, Optional

from psycopg2.extras import Json

from app.routers.projects import get_db_connection


def create_scenario(
    project_id: UUID,
    user_id: str,
    payload: Dict[str, Any],
) -> Dict[str, Any]:
    sql = """
        INSERT INTO public.project_scenarios (
            project_id, user_id,
            name, description, is_baseline, parameters
        )
        VALUES (%s, %s, %s, %s, %s, %s)
        RETURNING
            id, project_id, user_id,
            name, description, is_baseline,
            parameters, created_at, updated_at;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(
                sql,
                (
                    str(project_id),
                    user_id,
                    payload["name"],
                    payload.get("description"),
                    payload.get("is_baseline", False),
                    Json(payload.get("parameters", {})),
                ),
            )
            row = cur.fetchone()
            conn.commit()
            return dict(zip([d[0] for d in cur.description], row))


def list_scenarios(
    project_id: UUID,
    user_id: str,
) -> List[Dict[str, Any]]:
    sql = """
        SELECT
            id, project_id, user_id,
            name, description, is_baseline,
            parameters, created_at, updated_at
        FROM public.project_scenarios
        WHERE project_id = %s AND user_id = %s
        ORDER BY is_baseline DESC, created_at ASC;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            rows = cur.fetchall()
            cols = [d[0] for d in cur.description]
            return [dict(zip(cols, row)) for row in rows]


def get_scenario(
    project_id: UUID,
    scenario_id: UUID,
    user_id: str,
) -> Optional[Dict[str, Any]]:
    sql = """
        SELECT
            id, project_id, user_id,
            name, description, is_baseline,
            parameters, created_at, updated_at
        FROM public.project_scenarios
        WHERE project_id = %s AND id = %s AND user_id = %s;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), str(scenario_id), user_id))
            row = cur.fetchone()
            if not row:
                return None
            return dict(zip([d[0] for d in cur.description], row))


def update_scenario(
    project_id: UUID,
    scenario_id: UUID,
    user_id: str,
    payload: Dict[str, Any],
) -> Optional[Dict[str, Any]]:
    """
    Partial update; only apply fields present in payload.
    """
    fields = []
    values = []

    if "name" in payload:
        fields.append("name = %s")
        values.append(payload["name"])
    if "description" in payload:
        fields.append("description = %s")
        values.append(payload["description"])
    if "is_baseline" in payload:
        fields.append("is_baseline = %s")
        values.append(payload["is_baseline"])
    if "parameters" in payload:
        fields.append("parameters = %s")
        values.append(Json(payload["parameters"]))

    if not fields:
        return get_scenario(project_id, scenario_id, user_id)

    # updated_at
    fields.append("updated_at = now()")

    sql = f"""
        UPDATE public.project_scenarios
        SET {", ".join(fields)}
        WHERE project_id = %s AND id = %s AND user_id = %s
        RETURNING
            id, project_id, user_id,
            name, description, is_baseline,
            parameters, created_at, updated_at;
    """

    values.extend([str(project_id), str(scenario_id), user_id])

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, tuple(values))
            row = cur.fetchone()
            conn.commit()
            if not row:
                return None
            return dict(zip([d[0] for d in cur.description], row))


def delete_scenario(
    project_id: UUID,
    scenario_id: UUID,
    user_id: str,
) -> bool:
    sql = """
        DELETE FROM public.project_scenarios
        WHERE project_id = %s AND id = %s AND user_id = %s;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), str(scenario_id), user_id))
            deleted = cur.rowcount > 0
            conn.commit()
            return deleted--- ./db/models.py ---
# app/db/models.py

from pydantic import BaseModel
from typing import Optional
from datetime import datetime
from uuid import UUID
import os



JWT_SECRET = os.getenv("SUPABASE_JWT_SECRET")
ALGORITHM = "HS256"

print("JWT_SECRET loaded? ->", bool(JWT_SECRET))  


# Schema used for retrieving a saved project from the database
class ProjectDB(BaseModel):
    id: UUID
    user_id: UUID
    project_name: str
    description: Optional[str] = None
    start_year: int
    forecast_duration: int
    discount_rate: float # Numeric in PG, float in Python
    created_at: datetime
    updated_at: datetime
    
    class Config:
        orm_mode = True # Allows Pydantic to read ORM objects--- ./db/__init__.py ---
--- ./db/schemas.py ---
# app/db/schemas.py

from pydantic import BaseModel
from typing import Optional, Dict, Any, List
from datetime import datetime
from uuid import UUID

# ============================================================
# PROJECT Schemas
# ============================================================

class ProjectMetadata(BaseModel):
    project_name: str
    description: Optional[str] = None
    start_year: int
    forecast_duration: int
    discount_rate: float


class ProjectDB(BaseModel):
    id: UUID
    user_id: UUID
    project_name: str
    description: Optional[str] = None
    start_year: int
    forecast_duration: int
    discount_rate: float
    created_at: datetime
    updated_at: datetime
    
    class Config:
        from_attributes = True

# ============================================================
# MASTER DATA Schemas
# ============================================================

class MasterDataUploadStatus(BaseModel):
    id: UUID
    project_id: UUID
    user_id: UUID
    original_filename: str
    mime_type: str
    file_size: int
    status: str
    row_count: Optional[int] = None
    validation_errors: Optional[Dict[str, Any]] = None
    created_at: datetime--- ./main.py ---
# app/main.py (FINAL UPDATED VERSION)

from pathlib import Path
from dotenv import load_dotenv

# -------------------------------------------------------------------
# Load .env BEFORE importing anything that relies on environment vars
# -------------------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent.parent
load_dotenv(BASE_DIR / ".env")

# -------------------------------------------------------------------
# FastAPI + CORS
# -------------------------------------------------------------------
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

# Routers
from app.routers import projects
from app.master_data import router as master_data_router
from app.network_snapshot import router as network_snapshot_router
from app.scenarios import router as scenarios_router
from app.dashboards import router as dashboards_router




# -------------------------------------------------------------------
# FastAPI APP CONFIG
# -------------------------------------------------------------------
app = FastAPI(
    title="Mosianedi Investment API",
    description="API Gateway for RONET computation and scenario management.",
)


# -------------------------------------------------------------------
# MIDDLEWARE
# -------------------------------------------------------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all — adjust for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# -------------------------------------------------------------------
# ROUTES
# -------------------------------------------------------------------
app.include_router(
    projects.router,
    prefix="/api/v1/projects",
    tags=["Projects"],
)

app.include_router(
    master_data_router,
    prefix="/api/v1/projects",
    tags=["Master Data"],
)

app.include_router(
    network_snapshot_router, 
    prefix="/api/v1/projects", 
    tags=["Network"])


app.include_router(
    scenarios_router, 
    prefix="/api/v1/projects", 
    tags=["Scenarios"])

app.include_router(
    dashboards_router, 
    prefix="/api/v1/projects", 
    tags=["Dashboards"]
)

# -------------------------------------------------------------------
# ROOT PING / HEALTHCHECK
# -------------------------------------------------------------------
@app.get("/")
def read_root():
    return {
        "status": "ok",
        "service": "Mosianedi Investment API"
    }--- ./master_data/service.py ---
# app/master_data/service.py
from typing import Any, Dict, List, Optional
from uuid import UUID

from fastapi import HTTPException, UploadFile

from app.db.schemas import MasterDataUploadStatus
from .schemas import DataPreviewResponse
from .validation import (
    parse_master_data_file,
    parse_master_workbook,
    validate_flat_segments_df,
    validate_segments_sheet,
)
from . import repository


async def upload_master_data_service(
    project_id: UUID,
    user_id: str,
    upload_file: UploadFile,
) -> MasterDataUploadStatus:
    """
    Orchestrates upload + validation + workbook parsing + persistence.

    - For Excel workbooks:
        * parse all relevant sheets into workbook_payload (JSON)
        * validate the 'segments' sheet
    - For CSV:
        * treat the flat table as the segments sheet
        * optionally still store it in workbook_payload["segments"]
    """
    file_bytes = await upload_file.read()
    if not file_bytes:
        raise HTTPException(status_code=400, detail="Uploaded file is empty.")

    mime_type = upload_file.content_type or "application/octet-stream"
    file_size = len(file_bytes)
    original_filename = upload_file.filename or "uploaded_file"
    name_lower = original_filename.lower()

    status: str
    row_count: Optional[int]
    validation_errors: Dict[str, Any]
    workbook_payload: Optional[Dict[str, Any]] = None

    try:
        # -------- Excel workbook path --------
        if name_lower.endswith((".xlsx", ".xls")):
            # Parse full workbook into JSON-friendly payload
            workbook_payload = parse_master_workbook(file_bytes, original_filename)

            # Validate the segments sheet from that payload
            status, row_count, validation_errors = validate_segments_sheet(
                workbook_payload
            )

        # -------- CSV / flat table path --------
        else:
            df = parse_master_data_file(file_bytes, original_filename)
            status, row_count, validation_errors = validate_flat_segments_df(df)

            # Optionally store the CSV as a "segments" sheet in the payload
            df_norm = df.fillna("")
            df_norm.columns = [str(c).strip().lower() for c in df_norm.columns]
            workbook_payload = {
                "segments": df_norm.to_dict(orient="records"),
            }

    except HTTPException as exc:
        # Parsing/format errors – treat as failed but still store file & error
        status = "failed"
        row_count = None
        validation_errors = {
            "parsing_error": getattr(exc, "detail", None)
            or "File could not be parsed."
        }
        workbook_payload = None

    except Exception as e:
        status = "failed"
        row_count = None
        validation_errors = {
            "validation_exception": f"Unexpected error during validation: {str(e)}"
        }
        workbook_payload = None

    # ---- PERSIST ----
    record = repository.insert_master_data_upload(
        project_id=project_id,
        user_id=user_id,
        original_filename=original_filename,
        mime_type=mime_type,
        file_size=file_size,
        storage_strategy="inline",
        file_bytes=file_bytes,
        status=status,
        row_count=row_count,
        validation_errors=validation_errors if validation_errors else None,
        # Store parsed workbook JSON (if any)
        workbook_payload=workbook_payload if workbook_payload else None,
    )

    return MasterDataUploadStatus(**record)


def get_last_master_data_upload_service(
    project_id: UUID,
    user_id: str,
) -> MasterDataUploadStatus:
    record = repository.fetch_last_master_data_upload(project_id, user_id)
    if not record:
        raise HTTPException(
            status_code=404,
            detail="No master data uploads found for this project.",
        )
    return MasterDataUploadStatus(**record)


def get_all_master_data_uploads_service(
    project_id: UUID,
    user_id: str,
) -> List[MasterDataUploadStatus]:
    rows = repository.fetch_all_master_data_uploads(project_id, user_id)
    return [MasterDataUploadStatus(**r) for r in rows]


def _preview_from_blob(file_bytes: bytes, filename: str) -> DataPreviewResponse:
    """
    Preview helper: parses the first sheet / CSV file and returns the first 50 rows.
    """
    df = parse_master_data_file(file_bytes, filename)
    total_rows = len(df.index)
    preview_df = df.head(50).fillna("")
    preview_data = preview_df.to_dict("records")
    return DataPreviewResponse(
        preview_data=preview_data,
        total_rows=total_rows,
        columns=list(df.columns),
    )


def get_master_data_preview_latest_service(
    project_id: UUID,
    user_id: str,
) -> DataPreviewResponse:
    blob = repository.fetch_upload_blob_latest(project_id, user_id)
    if not blob:
        raise HTTPException(
            status_code=404,
            detail="No master data upload file found for this project.",
        )
    file_bytes, filename = blob
    if not file_bytes:
        raise HTTPException(
            status_code=500,
            detail="File blob was empty after retrieval.",
        )
    return _preview_from_blob(file_bytes, filename)


def get_master_data_preview_by_upload_service(
    project_id: UUID,
    upload_id: UUID,
    user_id: str,
) -> DataPreviewResponse:
    blob = repository.fetch_upload_blob_by_id(project_id, upload_id, user_id)
    if not blob:
        raise HTTPException(
            status_code=404,
            detail="Upload not found for preview.",
        )
    file_bytes, filename = blob
    if not file_bytes:
        raise HTTPException(
            status_code=500,
            detail="File blob was empty after retrieval.",
        )
    return _preview_from_blob(file_bytes, filename)--- ./master_data/__init__.py ---
# app/master_data/__init__.py
from .router import router

__all__ = ["router"]--- ./master_data/schemas.py ---
# app/master_data/schemas.py
from typing import Any, Dict, List
from pydantic import BaseModel


class DataPreviewResponse(BaseModel):
    preview_data: List[Dict[str, Any]]
    total_rows: int
    columns: List[str]--- ./master_data/router.py ---
# app/master_data/router.py
from typing import List
from uuid import UUID

from fastapi import APIRouter, Depends, File, UploadFile

from app.db.schemas import MasterDataUploadStatus
from app.routers.projects import get_current_user_id
from .schemas import DataPreviewResponse
from . import service

router = APIRouter()


# POST /api/v1/projects/{project_id}/master-data/upload
@router.post(
    "/{project_id}/master-data/upload",
    response_model=MasterDataUploadStatus,
)
async def upload_master_data(
    project_id: UUID,
    file: UploadFile = File(...),
    user_id: str = Depends(get_current_user_id),
):
    return await service.upload_master_data_service(
        project_id=project_id,
        user_id=user_id,
        upload_file=file,
    )


# GET /api/v1/projects/{project_id}/master-data/last-upload
@router.get(
    "/{project_id}/master-data/last-upload",
    response_model=MasterDataUploadStatus,
)
async def get_last_master_data_upload(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.get_last_master_data_upload_service(project_id, user_id)


# ✅ History: list all uploads for this project+user
@router.get(
    "/{project_id}/master-data/uploads",
    response_model=List[MasterDataUploadStatus],
)
async def get_all_master_data_uploads(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.get_all_master_data_uploads_service(project_id, user_id)


# GET /api/v1/projects/{project_id}/master-data/preview  (latest file)
@router.get(
    "/{project_id}/master-data/preview",
    response_model=DataPreviewResponse,
)
async def get_master_data_preview_latest(
    project_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.get_master_data_preview_latest_service(project_id, user_id)


# ✅ Preview a specific upload by ID (history picker)
@router.get(
    "/{project_id}/master-data/uploads/{upload_id}/preview",
    response_model=DataPreviewResponse,
)
async def get_master_data_preview_by_upload(
    project_id: UUID,
    upload_id: UUID,
    user_id: str = Depends(get_current_user_id),
):
    return service.get_master_data_preview_by_upload_service(
        project_id, upload_id, user_id
    )--- ./master_data/repository.py ---
# app/master_data/repository.py
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID

import psycopg2
from psycopg2.extras import Json

from app.routers.projects import get_db_connection


def insert_master_data_upload(
    project_id: UUID,
    user_id: str,
    original_filename: str,
    mime_type: str,
    file_size: int,
    storage_strategy: str,
    file_bytes: bytes,
    status: str,
    row_count: Optional[int],
    validation_errors: Optional[Dict[str, Any]],
    workbook_payload: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    Insert a new row into master_data_uploads and return the record.

    workbook_payload:
        Parsed multi-sheet workbook (segments, costs, iri, etc.) stored as JSONB.
        Can be None for simple CSV uploads.
    """
    sql = """
        INSERT INTO public.master_data_uploads (
            project_id, user_id,
            original_filename, mime_type, file_size,
            storage_strategy, file_blob,
            status, row_count, validation_errors,
            workbook_payload
        )
        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
        RETURNING
            id, project_id, user_id,
            original_filename, mime_type, file_size,
            status, row_count, validation_errors, created_at;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(
                sql,
                (
                    str(project_id),
                    user_id,
                    original_filename,
                    mime_type,
                    file_size,
                    storage_strategy,
                    psycopg2.Binary(file_bytes),
                    status,
                    row_count,
                    Json(validation_errors) if validation_errors else None,
                    Json(workbook_payload) if workbook_payload else None,
                ),
            )
            row = cur.fetchone()
            columns = [d[0] for d in cur.description]
            record = dict(zip(columns, row))
            conn.commit()
            return record


def fetch_last_master_data_upload(
    project_id: UUID,
    user_id: str,
) -> Optional[Dict[str, Any]]:
    """
    Return the latest upload for a project+user, or None.
    """
    sql = """
        SELECT
            id, project_id, user_id,
            original_filename, mime_type, file_size,
            status, row_count, validation_errors, created_at
        FROM public.master_data_uploads
        WHERE project_id = %s AND user_id = %s
        ORDER BY created_at DESC
        LIMIT 1;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            row = cur.fetchone()
            if not row:
                return None
            columns = [d[0] for d in cur.description]
            return dict(zip(columns, row))


def fetch_all_master_data_uploads(
    project_id: UUID,
    user_id: str,
) -> List[Dict[str, Any]]:
    """
    Return all uploads (history) for a project+user, newest → oldest.
    """
    sql = """
        SELECT
            id, project_id, user_id,
            original_filename, mime_type, file_size,
            status, row_count, validation_errors, created_at
        FROM public.master_data_uploads
        WHERE project_id = %s AND user_id = %s
        ORDER BY created_at DESC;
    """

    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            rows = cur.fetchall()
            if not rows:
                return []
            columns = [d[0] for d in cur.description]
            return [dict(zip(columns, r)) for r in rows]


def fetch_upload_blob_latest(
    project_id: UUID,
    user_id: str,
) -> Optional[Tuple[bytes, str]]:
    """
    Return (file_bytes, filename) for the latest upload, or None.
    """
    sql = """
        SELECT file_blob, original_filename
        FROM public.master_data_uploads
        WHERE project_id = %s AND user_id = %s
        ORDER BY created_at DESC
        LIMIT 1;
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(project_id), user_id))
            row = cur.fetchone()
            if not row:
                return None

            file_blob_raw, filename = row
            file_bytes = (
                file_blob_raw.tobytes()
                if isinstance(file_blob_raw, memoryview)
                else file_blob_raw
            )
            return file_bytes, filename


def fetch_upload_blob_by_id(
    project_id: UUID,
    upload_id: UUID,
    user_id: str,
) -> Optional[Tuple[bytes, str]]:
    """
    Return (file_bytes, filename) for a specific upload, or None.
    """
    sql = """
        SELECT file_blob, original_filename
        FROM public.master_data_uploads
        WHERE id = %s AND project_id = %s AND user_id = %s
        LIMIT 1;
    """
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, (str(upload_id), str(project_id), user_id))
            row = cur.fetchone()
            if not row:
                return None

            file_blob_raw, filename = row
            file_bytes = (
                file_blob_raw.tobytes()
                if isinstance(file_blob_raw, memoryview)
                else file_blob_raw
            )
            return file_bytes, filename--- ./master_data/validation.py ---
# app/master_data/validation.py
from typing import Dict, Any, Optional, Set, Tuple, List
from io import BytesIO

import pandas as pd
from fastapi import HTTPException

# -------------------------------------------------
# Expected workbook structure
# -------------------------------------------------

# We validate only the core "segments" sheet for now.
REQUIRED_SEGMENT_COLUMNS: Set[str] = {
    "segment_id",
    "road_id",
    "road_class",
    "surface_type",
    "length_km",
}

# Other sheets are optional but expected in the Mosianedi template.
EXPECTED_SHEETS: List[str] = [
    "segments",
    "network_type_surface",
    "network_length",
    "asset_value",
    "iri_defaults",
    "road_costs",
]

# -------------------------------------------------
# Parsing helpers
# -------------------------------------------------


def parse_master_data_file(file_bytes: bytes, filename: str) -> pd.DataFrame:
    """
    Helper used for previewing the FIRST sheet only.

    - For Excel workbooks: return the first sheet as a DataFrame
      (typically the 'segments' sheet in our template).
    - For CSV: parse as a flat table.
    """
    buffer = BytesIO(file_bytes)
    name = filename.lower()

    try:
        if name.endswith((".xlsx", ".xls")):
            xls = pd.ExcelFile(buffer)
            first_sheet = xls.sheet_names[0]
            df = xls.parse(sheet_name=first_sheet)
            return df
        elif name.endswith(".csv"):
            return pd.read_csv(buffer)
        else:
            raise HTTPException(status_code=400, detail="Invalid file type.")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Could not parse file: {str(e)}")


# -------------------------------------------------
# Workbook parser
# -------------------------------------------------


def parse_master_workbook(file_bytes: bytes, filename: str) -> Dict[str, Any]:
    """
    Parse our Mosianedi multi-sheet workbook into a JSON-friendly structure.

    Returns a dict like:
    {
        "segments": [ {...}, {...}, ... ],
        "network_type_surface": [ ... ],
        ...
        "_sheet_errors": { "sheet_name": "error message", ... }
    }

    For CSV uploads, this returns {} (no workbook structure).
    """
    name = filename.lower()

    # CSV or other non-Excel ⇒ no workbook
    if not name.endswith((".xlsx", ".xls")):
        return {}

    try:
        xls = pd.ExcelFile(BytesIO(file_bytes))
    except Exception as exc:
        # Don't kill the upload; just record that the workbook couldn't be opened.
        return {"_workbook_error": f"Could not open workbook: {exc}"}

    payload: Dict[str, Any] = {}
    sheet_errors: Dict[str, str] = {}

    for sheet in EXPECTED_SHEETS:
        if sheet not in xls.sheet_names:
            # Sheet is optional — just skip if not present
            continue

        try:
            df = xls.parse(sheet_name=sheet)

            # Normalise column names: lowercase + trimmed
            df.columns = [str(c).strip().lower() for c in df.columns]

            # Replace NaNs with empty strings to keep JSON clean
            df = df.fillna("")

            payload[sheet] = df.to_dict(orient="records")
        except Exception as exc:
            sheet_errors[sheet] = str(exc)

    if sheet_errors:
        payload["_sheet_errors"] = sheet_errors

    return payload


# -------------------------------------------------
# Validation helpers (segments only)
# -------------------------------------------------


def _validate_segments_df(df: pd.DataFrame) -> Tuple[str, Optional[int], Dict[str, Any]]:
    """
    Internal helper to validate that a segments-style DataFrame
    has the required columns.
    """
    row_count: Optional[int] = len(df.index)
    validation_errors: Dict[str, Any] = {}

    cols_lower = {c.lower() for c in df.columns}
    missing = [col for col in REQUIRED_SEGMENT_COLUMNS if col not in cols_lower]

    if missing:
        validation_errors["missing_columns"] = missing
        return "failed", row_count, validation_errors

    return "validated", row_count, validation_errors


def validate_flat_segments_df(df: pd.DataFrame) -> Tuple[str, Optional[int], Dict[str, Any]]:
    """
    Validate a flat (CSV / single-sheet) table as if it were the segments sheet.
    """
    return _validate_segments_df(df)


def validate_segments_sheet(
    workbook_payload: Dict[str, Any]
) -> Tuple[str, Optional[int], Dict[str, Any]]:
    """
    Validate the 'segments' sheet inside a parsed workbook payload.

    Expects workbook_payload["segments"] to be a list of dict rows.
    """
    if "segments" not in workbook_payload:
        return "failed", None, {"missing_sheet": "segments"}

    df = pd.DataFrame(workbook_payload["segments"])
    return _validate_segments_df(df)